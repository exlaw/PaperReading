## 论文阅读

![](https://img.shields.io/badge/PaperNumber-87-brightgreen) ![](https://img.shields.io/github/last-commit/exlaw/PaperReading?color=blue) 

个人论文阅读笔记，记录了所有读过的论文总结，基本每天更新。

- [论文阅读](#----)
  * [Exploring Auxiliary Reasoning Tasks for Task-oriented Dialog Systems with Meta Cooperative Learning](#exploring-auxiliary-reasoning-tasks-for-task-oriented-dialog-systems-with-meta-cooperative-learning)
  * [Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing](#awakening-latent-grounding-from-pretrained-language-models-for-semantic-parsing)
  * [Value-Agnostic Conversational Semantic Parsing](#value-agnostic-conversational-semantic-parsing)
  * [Vocabulary Learning via Optimal Transport for Neural Machine Translation](#vocabulary-learning-via-optimal-transport-for-neural-machine-translation)
  * [SMBOP Semi-autoregressive Bottom-up Semantic Parsing](#smbop-semi-autoregressive-bottom-up-semantic-parsing)
  * [ConSERT A Contrastive Framework for Self-Supervised Sentence Representation Transfer](#consert-a-contrastive-framework-for-self-supervised-sentence-representation-transfer)
  * [SimCSE Simple Contrastive Learning of Sentence Embeddings](#simcse-simple-contrastive-learning-of-sentence-embeddings)
  * [Keep the Structure: A Latent Shift-Reduce Parser for Semantic Parsing](#keep-the-structure--a-latent-shift-reduce-parser-for-semantic-parsing)
  * [MEDA Meta-Learning with Data Augmentation for Few-Shot Text Classification](#meda-meta-learning-with-data-augmentation-for-few-shot-text-classification)
  * [All NLP Tasks Are Generation Tasks: A General Pretraining Framework](#all-nlp-tasks-are-generation-tasks--a-general-pretraining-framework)
  * [Joint Verification and Reranking for Open Fact Checking Over Tables](#joint-verification-and-reranking-for-open-fact-checking-over-tables)
  * [Towards Table-to-Text Generation with Numerical Reasoning](#towards-table-to-text-generation-with-numerical-reasoning)
  * [Towards Robustness of Text-to-SQL Models against Synonym Substitution](#towards-robustness-of-text-to-sql-models-against-synonym-substitution)
  * [LGESQL Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations](#lgesql-line-graph-enhanced-text-to-sql-model-with-mixed-local-and-non-local-relations)
  * [Optimizing Deeper Transformers on Small Datasets](#optimizing-deeper-transformers-on-small-datasets)
  * [From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding](#from-paraphrasing-to-semantic-parsing--unsupervised-semantic-parsing-via-synchronous-semantic-decoding)
  * [Span-based Semantic Parsing for Compositional Generalization](#span-based-semantic-parsing-for-compositional-generalization)
  * [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](#compositional-generalization-and-natural-language-variation--can-a-semantic-parsing-approach-handle-both-)
  * [On the Sentence Embeddings from Pre-trained Language Models](#on-the-sentence-embeddings-from-pre-trained-language-models)
  * [All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text](#all-that-s--human--is-not-gold--evaluating-human-evaluation-of-generated-text)
  * [KILT a Benchmark for Knowledge Intensive Language Tasks](#kilt-a-benchmark-for-knowledge-intensive-language-tasks)
  * [WIKITABLET A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections](#wikitablet-a-large-scale-data-to-text-dataset-for-generating-wikipedia-article-sections)
  * [Describing a Knowledge Base](#describing-a-knowledge-base)
  * [GenWiki A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation](#genwiki-a-dataset-of-13-million-content-sharing-text-and-graphs-for-unsupervised-graph-to-text-generation)
  * [WikiGraphs A Wikipedia Text Knowledge Graph Paired Dataset](#wikigraphs-a-wikipedia-text-knowledge-graph-paired-dataset)
  * [Text-to-Table A New Way of Information Extraction](#text-to-table-a-new-way-of-information-extraction)
  * [BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension](#bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension)
  * [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](#exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer)
  * [Improving Language Understanding by Generative Pre-Training](#improving-language-understanding-by-generative-pre-training)
  * [Language Models are unsupervised multitask learners](#language-models-are-unsupervised-multitask-learners)
  * [Language models are few shot learners](#language-models-are-few-shot-learners)
  * [Its Not Just Size That Matters Small Language Models Are Also Few-Shot Learners](#its-not-just-size-that-matters-small-language-models-are-also-few-shot-learners)
  * [KnowPrompt Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction](#knowprompt-knowledge-aware-prompt-tuning-with-synergistic-optimization-for-relation-extraction)
  * [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](#exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inference)
  * [Bidirectional Transition-Based Dependency Parsing](#bidirectional-transition-based-dependency-parsing)
  * [Constrained Language Models Yield Few-Shot Semantic Parsers](#constrained-language-models-yield-few-shot-semantic-parsers)
  * [HTLM: Hyper-Text Pre-Training and Prompting of Language Models](#htlm--hyper-text-pre-training-and-prompting-of-language-models)
  * [Pre-train Prompt and Predict A Systematic Survey of Prompting Methods in Natural Language Processing](#pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processing)
  * [NumER A Fine-Grained Numeral Entity Recognition Dataset](#numer-a-fine-grained-numeral-entity-recognition-dataset)
  * [Asynchronous Bidirectional Decoding for Neural Machine Translation](#asynchronous-bidirectional-decoding-for-neural-machine-translation)
  * [Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation](#agreement-based-joint-training-for-bidirectional-attention-based-neural-machine-translation)
  * [Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing](#data-augmentation-with-hierarchical-sql-to-question-generation-for-cross-domain-text-to-sql-parsing)
  * [Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization](#exploring-underexplored-limitations-of-cross-domain-text-to-sql-generalization)
  * [Natural SQL: Making SQL Easier to Infer from Natural Language Specifications](#natural-sql--making-sql-easier-to-infer-from-natural-language-specifications)
  * [TinyBERT Distilling BERT for Natural Language Understanding](#tinybert-distilling-bert-for-natural-language-understanding)
  * [UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING](#unsupervised-data-augmentation-for-consistency-training)
  * [GRAPPA GRAMMAR-AUGMENTED PRE-TRAINING FOR TABLE SEMANTIC PARSING](#grappa-grammar-augmented-pre-training-for-table-semantic-parsing)
  * [Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training](#learning-contextual-representations-for-semantic-parsing-with-generation-augmented-pre-training)
  * [Calibrate Before Use Improving Few-Shot Performance of Language Models](#calibrate-before-use-improving-few-shot-performance-of-language-models)
  * [Zero-Shot Text-to-SQL Learning with Auxiliary Task](#zero-shot-text-to-sql-learning-with-auxiliary-task)
  * [Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning](#leveraging-table-content-for-zero-shot-text-to-sql-with-meta-learning)
  * [Prefix-to-SQL Text-to-SQL Generation from Incomplete User Questions](#prefix-to-sql-text-to-sql-generation-from-incomplete-user-questions)
  * [DoT An efficient Double Transformer for NLP tasks with tables](#dot-an-efficient-double-transformer-for-nlp-tasks-with-tables)
  * [Understanding tables with intermediate pre-training](#understanding-tables-with-intermediate-pre-training)
  * [Re-examining the Role of Schema Linking in Text-to-SQL](#re-examining-the-role-of-schema-linking-in-text-to-sql)
  * [Translate & Fill Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data](#translate---fill-improving-zero-shot-multilingual-semantic-parsing-with-synthetic-data)
  * [Context-Aware Attention Network for Image-Text Retrieval](#context-aware-attention-network-for-image-text-retrieval)
  * [Table2Vec: Neural Word and Entity Embeddings for Table Population and Retrieval](#table2vec--neural-word-and-entity-embeddings-for-table-population-and-retrieval)
  * [CPT COLORFUL PROMPT TUNING FOR PRE-TRAINED VISION-LANGUAGE MODELS](#cpt-colorful-prompt-tuning-for-pre-trained-vision-language-models)
  * [NSP-BERT A Prompt-based Zero-Shot Learner Through an Original Pre-training Task Next Sentence Prediction](#nsp-bert-a-prompt-based-zero-shot-learner-through-an-original-pre-training-task-next-sentence-prediction)
  * [TAPEX Table Pre-training via Learning a Neural SQL Executor](#tapex-table-pre-training-via-learning-a-neural-sql-executor)
  * [On the Importance of Word Order Information in Cross-lingual Sequence Labeling](#on-the-importance-of-word-order-information-in-cross-lingual-sequence-labeling)
  * [Multiplicative Position-aware Transformer Models for Language Understanding](#multiplicative-position-aware-transformer-models-for-language-understanding)
  * [Semi-Supervised Learning for Neural Machine Translation](#semi-supervised-learning-for-neural-machine-translation)
  * [CONDITIONAL SET GENERATION USING SEQ2SEQ MODELS](#conditional-set-generation-using-seq2seq-models)
  * [ELECTRA PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS](#electra-pre-training-text-encoders-as-discriminators-rather-than-generators)
  * [End-to-End Object Detection with Transformers](#end-to-end-object-detection-with-transformers)
  * [PIX2SEQ A LANGUAGE MODELING FRAMEWORK FOR OBJECT DETECTION](#pix2seq-a-language-modeling-framework-for-object-detection)
  * [TextAttack A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP](#textattack-a-framework-for-adversarial-attacks--data-augmentation--and-adversarial-training-in-nlp)
  * [BERT-ATTACK Adversarial Attack Against BERT Using BERT](#bert-attack-adversarial-attack-against-bert-using-bert)
  * [Combating Adversarial Misspellings with Robust Word Recognition](#combating-adversarial-misspellings-with-robust-word-recognition)
  * [SeaD End-to-end Text-to-SQL Generation with Schema-aware Denoising](#sead-end-to-end-text-to-sql-generation-with-schema-aware-denoising)
  * [Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment](#measuring-and-improving-compositional-generalization-in-text-to-sql-via-component-alignment)
  * [RoBERTa A Robustly Optimized BERT Pretraining Approach](#roberta-a-robustly-optimized-bert-pretraining-approach)
  * [Disentangled Sequence to Sequence Learning for Compositional Generalization](#disentangled-sequence-to-sequence-learning-for-compositional-generalization)
  * [Incorporating Extra Knowledge to Enhance Word Embedding](#incorporating-extra-knowledge-to-enhance-word-embedding)
  * [Self-Supervised Learning for Contextualized Extractive Summarization](#self-supervised-learning-for-contextualized-extractive-summarization)
  * [Mind the Style of Text Adversarial and Backdoor Attacks Based on Text Style Transfer](#mind-the-style-of-text-adversarial-and-backdoor-attacks-based-on-text-style-transfer)
  * [A MUTUAL INFORMATION MAXIMIZATION PERSPECTIVE OF LANGUAGE REPRESENTATION LEARNING](#a-mutual-information-maximization-perspective-of-language-representation-learning)
  * [Unsupervised Pretraining for Sequence to Sequence Learning](#unsupervised-pretraining-for-sequence-to-sequence-learning)
  * [Transformer-XL Attentive Language Models Beyond a Fixed-Length Context](#transformer-xl-attentive-language-models-beyond-a-fixed-length-context)
  * [XLNet Generalized Autoregressive Pretraining for Language Understanding](#xlnet-generalized-autoregressive-pretraining-for-language-understanding)
  * [Understanding Back-Translation at Scale](#understanding-back-translation-at-scale)
  * [SQuAD: 100,000+ Questions for Machine Comprehension of Text](#squad--100-000--questions-for-machine-comprehension-of-text)
  * [Know What You Don’t Know: Unanswerable Questions for SQuAD](#know-what-you-don-t-know--unanswerable-questions-for-squad)
  * [The Power of Prompt Tuning for Low-Resource Semantic Parsing](#the-power-of-prompt-tuning-for-low-resource-semantic-parsing)
  * [CLINE Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](#cline-contrastive-learning-with-semantic-negative-examples-for-natural-language-understanding)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>


**所有Tag: 方便检索**

* Tag:自监督学习
* Tag:文档摘要
* Tag:词向量
* Tag:语义解析
* Tag:Text2SQL
* Tag:Prompt
* Tag:Text2Table
* Tag:对话系统
* Tag:元学习
* Tag:预训练模型
* Tag:机器翻译
* Tag:文本分类
* Tag:事实验证
* Tag:表格相关
* Tag:Data2Text
* Tag:对抗学习
* Tag:图网络
* Tag:优化方法
* Tag:外部知识引入
* Tag:Table2Text
* Tag:数据集
* Tag:半监督学习
* Tag:视觉相关
* Tag:Transformer
* Tag:问答

### Exploring Auxiliary Reasoning Tasks for Task-oriented Dialog Systems with Meta Cooperative Learning 

Tag:对话系统  Tag:元学习

AAAI 2021

https://ojs.aaai.org/index.php/AAAI/article/view/17615

AAAI 2021 的一篇论文，做的是任务型对话系统。   任务型对话的目标是通过和用户的对话达到某种特定的目的。 具体到任务上，输入是用户每一轮的问题和知识库（KB），输出是系统产生的回答。
尽管近期的很多采用seq2seq的方法取得了不错的效果，但是 seq2seq的方法也存在着两个问题 1. 是很难在知识库上进行推断（reasoning), 这导致了系统容易产生无关的回答。 2 是seq2seq方法很容易忽略语境信息，这导致了系统经常忽略关键信息。 

针对以上问题，本文采用了多任务学习的方式对seq2seq方法的不足进行了一些修正，新增了两个辅助任务，分别是 KB reasoning task 和 dialogue reasoning task。 并且采用了一个 meta cooperative learning 的方式来对三个任务进行统一的学习。

本文的baseline model 采用了经典模型，包括了dialogue encoder, dialogue memory(用来保存历史对话语句和状态)， KB Memory(用来保存历史知识库信息和状态)，dialogue decoder（用来产生最后的结果）。  输出方式采用类似 copy net的方式进行，用动态概率控制是从词汇表中选择、KB中选择还是历史对话中选择词汇。
第一个辅助任务是KB reasoning, 不使用 dialogue memory， 增加一个新的 KB reasoning network模块，目标是更好的学习  KB Memory， 最终的loss是和 标准主网络输出logit的KL散度比较。
第二个辅助任务是 dialogue reasoning, 不使用 KB memory, 新增一个 dialogue  reasoning 模块，目标是更好的学习 dialogue memory, 最终的loss 是和标准主网络logit的KL散度比较。
由于两个辅助任务的网络loss都是和主网络的KL散度比较，所有自然就有了梯度更新顺序问题，本文采用的算法是类似MAML的一个meta learning思想，取名为 meta cooperative learning 。

从最终的实验效果来看，在 CamRest， In-Car Assistant  和 Multi-WOZ 的各项指标上都取得了最优效果。 

### Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing 

Tag:语义解析

ACL2021 findings

https://aclanthology.org/2021.findings-acl.100.pdf

探索 大规模预训练模型 grounding capabilities  的工作比较少，本文就来探索  的 grounding capabilities , 具体来说就是 token和概念的对应关系，找到句子中每个token和相应概念的对应关系就是  grounding， 比如 text-to-sql 中的找到文本和数据库schema， 知识库 entity 之间的对应关系。  本文通过应用一个简单的 erase-Awaken 机制，在不需要标注（token级别）的情况下学到了grounding信息，在后续的实验中，应用到 text-to-sql 领域，可以取得最高9.8%的提升。

方法分成三个步骤，第一个步骤是训练一个概念预测模块，这个模块的目的是判断一个句子是否提到了某个概念。 在 text-to-sql 领域中，对应的是数据库中的schema信息是否在 SQL语句中出现，这个标注信息已经存在，不需要再进行额外的标注。  第二个步骤是 擦除token,  然后来判断擦除token后对某个概念的 预测分数差别，如果差别很大，那么说明这个token对这个概念能产生非常大的影响，否则就不会产生影响，这样也就产生了一个相关矩阵。 第三个步骤是用上一步产生的相关矩阵作为监督信号再去训练一个Latent Grounding的预测模块（是根据实验效果直接使用相关矩阵作为预测结果效果并不好，本文认为Latent Grounding的预测模块能够加大不同模块之间的差别）。

实验主要分成两个部分，第一个部分是对 Grounding 的预测准确率判断，和之前人工标注的Grounding信息进行对比，根据作者的实验结果，效果大幅度超过了其他的模型，并且能够取得和完全监督学习类似的效果。 第二个部分是应用到 text-to-sql任务中作为 linking模块的数据，对SLSQL 等比较简单的模型都能产生相对较大的提升。

### Value-Agnostic Conversational Semantic Parsing 

Tag:语义解析

ACL 2021

https://aclanthology.org/2021.acl-long.284.pdf

对话场景下的 semantic parsing 目前大多数做法都把之前轮次的对话全部都进行编码，这样会导致计算效率低，并且产生一些没有必要的依赖。 本文认为，只需要之前预测结果 program 的类型信息，不需要具体的值 （Value-Agnostic）就可以产生非常好的效果。 本文的贡献： 1.  使用了一个紧编码，这个编码只包含了历史program 的 type， 但是本文仍然认为这个信息是足够的。  2.  在 decode的时候把对function的预测和对于argument的预测做了独立，分别预测提升了计算效率，可以允许进行更大的 beam search 提高最后的结果。

具体方法： 在编码的过程中，首先通过 transformer 模型把句子编码，然后把句子的编码和对之前program的类型信息作一个attention, 产生对句子的编码。  在解码过程中，对函数序列的预测和对于参数的预测独立开来，不再是作一个 tree 结构预测或者是序列预测， 有点类似于分层的序列预测。  其中具体的参数值设置了四个来源： 之前函数调用的引用，静态词表中的值，句子中复制的字符串，实体提取器中得到的实体。  最终的实验结果中，在SMCALFLOW 和 TREEDST 两个数据集中分别提升了 7.3% 和 10.6%。

### Vocabulary Learning via Optimal Transport for Neural Machine Translation

Tag:机器翻译

ACL 2021 的最佳论文

https://aclanthology.org/2021.acl-long.571.pdf

token 词表的选择在NLP中非常重要， 传统的词表选择有基于character和基于word的，基于character的词表选择长度太长，学习难度大， 基于word的词表容易在 rare word熵出现 OOV 的问题， 所以之前的工作也提出了BPE的方法选择的高频的sub-words或者sub-word pieces来做更好的词表，这样使用更小的词表可以压缩数据，减小数据集的熵。 但是使用 BPE 方法的时候选用多大的词表是一个 trade-off问题，词表的增大可以使得语料库熵减小，但是会导致token sparsity， 之前使用 tral learning 需要遍历全部的情况，非常耗费时间，为此，作者提出了一种基于最优传输的词汇学习方法，简称VOLT。

具体来说，作者提出了 MUV（marginal utility of vocabularization ）的概念， 即词表的边际效应，整个算法的 优化目标就是 MUV， 首先作者进行了一些简单的实验，发现 MUV 曲线和最终的BLEU值相关程度非常高，说明优化MUV是有价值的。  直观地说，作者将词汇表构造想象为一个传输过程，将字符传输到数量高达S[t]的候选token中。由于字符数是固定的，并非所有候选token都能获得足够的字符。每个传输矩阵都可以通过收集带有字符的token来构建词汇表，最优传输的目标是找到一个传输矩阵来最小化传输成本，即在作者的设置中为负熵。 后面的数学处理相对比较硬核，就先不详细介绍了。

在实验部分，在多个数据集上都能取得很稳定的效果，在词表大小减小为1/3的情况下，BLEU值甚至能持平或者更好（高0.5%的点已经非常多了）。   总体来看， 本文基于optimal transport的思想，很新颖，并且实验的结果非常充分，solid。

### SMBOP Semi-autoregressive Bottom-up Semantic Parsing 

Tag:语义解析 Tag:Text2SQL

NAACL 2021 

https://arxiv.org/pdf/2010.12412.pdf

目前几乎大多数的 Text-to-SQL 方法采用的都是 top-down 的decoding 方法，也就是一种auto-regressive 的方法，从根节点开始，由上至下，由左至右逐渐decode成整个语法树。  在这个工作中，尝试了一种不同的decode方式，也就是一种 bottom up 的decode方式，具体含义就是在第t步去构造一个高度不超过t的 top-K 个子树。 这样做的优势在于，可以并行化的生成子树，使得算法的复杂度由线性变成了对数复杂度。 根据作者的实际数据，在SPIDER 数据集上的训练加速可以达到5倍，推断加速达到2.2倍。

在具体的方法上，作者重新对SQL的语法进行改写，形成了一套特殊的关系代数，和自然语言的形式相对更接近一点，更重要的是增加了keep操作，使得所有的SQL树都是平衡树（子树高度相同）。  后续的操作就是每一层的筛选，第一层是数据库的schema 和数据库中的值，对每种可能对匹配都去计算score,包括对一个子树的操作（unary）和对两个子树的操作（binary）， 形成所有候选后，根据score值选取 K 个进入下一层的计算，并且过程中还使用了 cross attention的方法去更新了子树的 向量表示。  loss值的计算是对标准子树的最大似然。

在实验结果上，本文在 SPIDER 数据集上的实验结果和RAT-SQL 类似， 只有 0.3%的点的损失，考虑到采用了Semi-autoregressive的情况下，已经是不错的结果了。 同时本文也分析了在 senmantic parsing的场景下，bottom-up方法和 top-down方法在理论上的差距不是很大。

### ConSERT A Contrastive Framework for Self-Supervised Sentence Representation Transfer 

Tag:自监督学习

ACL 2021

https://arxiv.org/pdf/2105.11741.pdf

BERT在对句子的编码中会出现崩塌的现象，即几乎所有的句子都会被编码到一个相对比较小的空间中，这导致了计算句子相关性的时候会把一些不太相关的句子也给出相对比较高的相似度。 具体的原因是BERT对于高频词的编码相对来说比较集中，其他的相对低频的词汇则很分散，这导致了对整个句子计算编码的时候会很大程度的受到高频词的影响。 

为了解决这个问题，作者提出了一个基于对比学习的方式，结合数据增强让BERT更好的学到句子相关性。 具体来说，本文使用了四种数据增强的方式，Adversarial Attack （打乱句子编码）， Token Shuffling （把句子中的Token打乱）， Cutoff （在句子中删除一些Token）, Dropout 
(把Embedding中的一些维度去掉)。  具体的学习loss和标准的对比学习loss是类似的，数据增强后的数据为正例，和原数据尽可能接近，其他的句子是负例，使距离尽可能远。 同时作者还增加了监督学习的设定，使用NLI任务进行fine-tune。

在实验结果上，在无监督设定和监督学习设定下，都在6 个STS 数据集上取得了最优效果，并且还在某些数据集上取得了非常大的提升。并且本文另一优势是fine-tune所需要的时间比较短， 单GPU几个小时就可以完成。

### SimCSE Simple Contrastive Learning of Sentence Embeddings 

Tag:自监督学习

https://arxiv.org/pdf/2104.08821.pdf

学习通用的对句子的编码在NLP任务中是非常重要的，在这篇文章中，作者在经典的语言模型比如 BERT 和 RoBERTa 中引入对比学习方法，很大程度的提升了对句子的编码表示。  具体的做法十分简单，把一个句子两次通过通过同一个 embedding 网络，但是两次embedding时的dropout值不同，这样产生的两个embedding就作为 positive pairs， 一个句子和其他的句子编码就是 negative pairs, 于是就可以进行标准的对比学习。  对于这种对比学习，可以理解成一种数据增强，这种数据增强方式大大增强了对于句子的表示。 作者也在文章中对比了其他的数据增强方式，比如删除，替换，mask词等，但是都是不如dropout都效果的。 同时作者也同时使用了 NLI 数据集中的数据作为监督学习的数据，把entailment 数据作为positive, Contradiction 的数据作为 negative。 更重的是，作者也从数学原理层面对于对比学习的有效性进行了一些解释，之前使用BERT的模型在句子编码上的效果不佳是因为词编码矩阵的奇异值只有几个非常大的值，其他的奇异值都接近0，通过数学推导也看出了对比学习有助于使得奇异值更加平坦。 

从实验效果看，在全部都7个语义相似度的数据集上都取得了最佳的效果，并且比之前的最佳都有着不小的进步，并且在7个迁移任务上也几乎都取得了最优的效果。  这篇文章和 ConSERT 的做法几乎是一样的，也几乎是同时期的工作，不过好像这篇文章暂时还没发表在会议上，但是更出名一点。

### Keep the Structure: A Latent Shift-Reduce Parser for Semantic Parsing 

Tag:语义解析

IJCAI-2021

https://www.ijcai.org/proceedings/2021/0532.pdf

传统的端到端的semantic parsing模型都把自然语言看成是一个整体结构，然而自然语言本身也是可以进行结构划分的，并且划分后的结构可以和要预测的logic form进行一一对应。  所以在这篇文章中提出了一个使用 shift-reduce 解析器的方法，具体来说，文章中设定了一个 splitter可以把自然语言划分成不同的span, 之后对于每个span使用一个base parser解析出其结构，然后组合起来和ground truth进行对比。
方法的细节上，对于Base Parser，就是一个经典的seq2seq2结构，输入是span 的text 部分，经过一个 GRU 编码，又经过一个GPU 解码输出 sub logic form。 对于 Splitter， 作者把Splitter 的输出定义为针对栈和输入text的一系列action,包括shift操作，reduce操作，finish操作，通过这些操作每次找到一个span,就使用一个 span semantic vector 替换原有句子中的span部分，然后进行下一轮的操作。 最终所有的操作形成一个 action sequence, 作者称之为 trajectory（轨迹）。 

之后是训练方法，由于 Splitter 和 Base Parser 是两个相对独立的步骤，所以作者先进行了Trajectory Search 过程，尽可能搜索出大量的可能正确的 Trajectory， 然后使用 baseline parser对搜索出的 Trajectory 进行预测，对能成功匹配的部分直接作为pair, 不能匹配的部分直接作为一个比较大pair, 使用这些pairs对baseline parser进行训练，对于 Splitter， 把Trajectory视为 隐变量，使用maximum marginal likelihood (MML) estimation  进行训练。  整个系统有冷启动问题，所以一开始先使用全部的数据集对baseline parser进行预训练，防止训练完全偏离。

在实验结果上，在Geoquery dataset没有取得SOTA，但是比其他的所有不使用BERT的方法效果都好（本文也没有使用BERT）， 在更加复杂的 WQ dataset 数据集上取得了最佳的效果。   总体来看，本文通过引入 Splitter提升了 Semantic Parsing 的可解释性。

### MEDA Meta-Learning with Data Augmentation for Few-Shot Text Classification 

Tag:元学习  Tag:文本分类

IJCAI-21 

https://www.ijcai.org/proceedings/2021/0541.pdf

元学习已经成了一个非常重要的来解决 few-shot learning 问题的手段，但是之前的方法基本上都在CV领域，很难直接迁移到 NLP 的任务上。 所以这篇文章提出了一个使用元学习配合数据增强的方式来解决NLP中的few-shot learning 的方法。 具体来说，本文的方法主要分成两个部分，第一个部分是 Ball Generator， 这个模块是在embedding 层面上增强了数据，因为原本的训练集数据只有 K-shot, 增强后可以大大增强学习效果。 具体的方法是把之前的空间看作是一个球，在中心和半径范围内通过生成算法随机生成数据。 第二个模块是Meta-learner ，这个模块是主要的学习部分，根据原本的K-shot数据和增强后的数据来学习，并且对query-set 中的数据进行预测，在这个模块，本文直接采用了Prototypical Networks  和Relation Networks 这样现有的网络解决方案。 最终loss由两个部分组成，第一个部分是 Ball Generator的loss，对于每个生成的embedding,都要和自己的中心尽可能接近，和其他的中心尽可能远。 第二个部分是 Meta-learner的，在query-set上的分类loss。 两个部分求和就是学习的目标。

在实验结果上，在 SNIPS 数据集和ARSC 数据集上都取得了SOTA的效果，并且对比实验也说明了数据增强方法和元学习的有效性。

### All NLP Tasks Are Generation Tasks: A General Pretraining Framework 

Tag:预训练模型

https://arxiv.org/pdf/2103.10360.pdf

目前的预训练模型有非常多种，包括autoregressive模型，autoencoding 模型，encoder-decoder 模型等等，但是并没有一类模型能在所有的NLPtask上（分类任务，条件生成任务，非条件生成任务）都能取得非常好的效果。 本文提出了GLM模型，希望能去解决这个问题，在多种类的任务上都能取得不错的效果，减少模型的选择问题。

具体来说，本文采用的预训练是Autoregressive Blank Infilling，是随机在文本中选择出 span, 然后用Autoregressive 的方式把这个span补全完整，这个任务结合了 autoencoding任务和 autoregressive任务，采用这个任务可以天然的在分类任务和生成任务都能取得还不错的效果。 不过之前也有SpanBERT做了类似的事情，本文的一个不同之处是本文找到了多个span, 然后把这些span做了打乱，然后放在原本序列后进行span补全任务。 这样生成的模型在做分类任务和生成任务都采用生成的模式来做， 在分类的任务的时候，会构造一个含有mask的句子，然后生成这个mask，根据生成的词汇来判断。 比如情分类似，会有一个 it is really [mask], 根据mask预测的词汇来进行判断（当然各种词汇的分类可能是另一个话题）。

在实验效果上，在NLU的GLUE数据集上基本能稳定比BERT好（同等参数量和数据量下）， 在 abstractive summarization 任务上取得了比UniLM 更好的效果，在Zero-shot language modeling 任务上取得了比GPT 模型更好的效果。

### Joint Verification and Reranking for Open Fact Checking Over Tables 

Tag:事实验证  Tag:表格相关

ACL 2021

https://aclanthology.org/2021.acl-long.529.pdf

结构信息对于fack checking 任务是一个非常重要的信息来源，然而之前的大多数工作工作都是仅仅采用了文本信息并且假设已经找到的正确的evidence。 在本文中，作者使用了结构化信息进行了 fack checking， 并且在是open domain 的设定下，即事先不知道对应的table，需要模型去寻找和学习。 

整个任务的定义是给一个问题和一个表格的集合，判断这个问题的真伪。 本文的方法首先找和问题最相关的K个table。 采用的方法其实就是非常经典的 TF_IDF 算法， 返回的 score最高的K个table。 在后续的分类中，需要把问题和table进行联合编码，首先再次把问题和table进行entity匹配，只找到最相关的3个column。 把选择table信息和问题拼接进入RoBERTa，取最后的 CLS-token 的编码值作为最后的编码结果， 为了能把各个表格的信息都能充分类用，后续还经过了一个cross attention层。  最终的分类层有两种策略，一种是 Joint reranking and verification， 即对每个编码经过分类层然后求和正则化作为最终logit, 然后求loss优化。 第二种是Ternary verification，即对每个table的编码单独进行验证，最终根据真伪的数量进行判断问题的真伪。

在实验结果上，作者首先验证了使用了TF-IDF算法的准确率，基本能有70-85的accuracy。 最终在作者的实验结果中，发现在open domain 设定下取得了结果已经可以和 close domain setting 下的去对比，仅仅比使用额外生成数据的模型效果差一点。  总结来看，本文应该是率先在open domain 的设定下做table-based fact check的，方向还相对比较初级，这个领域应该还是很多工作可以继续进行。

### Towards Table-to-Text Generation with Numerical Reasoning 

Tag:Data2Text Tag:表格相关

ACL 2021

https://aclanthology.org/2021.acl-long.115.pdf

近期的文本生成模型已经能够在生成对于结构化数据的描述文本上取得比较好的效果了。 但是目前仍然有一个非常大的挑战是产生一些需要数值推断的描述性问题。 针对这个问题，本文首先提出了一个新的数据集，数据集是由最近几年ACL会议的所有论文的表格和其描述组成的，由于论文中的描述一般都是由一些数值推断的，所以这个构造方式还比较合理。 同时作者还尝试了一些方法在这个新的数据集上的效果。

在数据集的构造的上，先用工具提取出了表格，然后筛选出有数字的描述，并且对所有的描述分成了三类，分别是数据描述，支持性描述和不相关的的描述，本文只选取了数据描述这个类别。
之后作者研究了如何对表格进行表示来更好的抽取推理信息，一个非常重要的步骤是对表格数据进行预计算， 把表格中的最大值，平均值，最小值等提前求出来，作为一个单独的 Pre-executed operation table, 然后把这个table和原始的table拼接起来线性化。

具体的方法上作者主要尝试了三种，第一种是templated-based method,从训练集上抽取模版，应用到测试集上， pointer generater 方法，使用 pointer generater 网络，不使用预训练模型。 第三种是使用预训练模型，同时设计了一种特殊的copy网络，主网络生成place-holder，比如<table_id>之类的，再由copy网络去选取具体的值。

在最终的实验结果上，  pointer generater 几乎不work, 使用预训练的GPT2模型取得了最好的效果，值得注意是copynet并没有带来提升。 总结来说，作者提出了这个数据集是有贡献意义的，但是并没有提出非常针对性的解决方案，说明这个数据集是有非常大的提升空间可以做的。

### Towards Robustness of Text-to-SQL Models against Synonym Substitution 

Tag:语义解析  Tag:Text2SQL  Tag:对抗学习

ACL 2021

https://aclanthology.org/2021.acl-long.195.pdf

虽然近期的很多 Text-to-SQL 工作取得了还不错的进展，模型的效果也在不断的提升，但是目前的目前非常依赖 schema-linking 这个机制，这就导致了系统十分不稳定并且容易受到攻击，当文本的表达被修改的时候，比如不使用 schema 中的词汇，那么 schema-linking 就会无效，在这宗情况下模型会受到多大的影响呢，本文就主要研究了这个问题。 本文针对Spider 数据集进行了二次标注，把自然语言表达中文本的大多数和数据库shcema 相同的本文都是用同义词进行替换，形成了一个新的Spider-Syn 数据集，并且针对这个新的数据集使用了两个相对比较简单的方法提升了模型的效果。

首先是数据标注的过程，使用了spider的训练集和验证集进行标注（因为测试集是不公开的），标注的过程并不是为了找到最优的攻击性case,而是尽量的模拟日常场景，因为在日常的用户的使用中就有可能会使用完全不同于数据库schema的词汇。针对不同的领域也给出了不同的同义词表方便标注使用。 

在方法上，主要使用了两个方法，第一个是Multi-Annotation Selection (MAS) ，使用标注的同义词汇同时进行训练。 第二个是Adversarial Training ，使用BERT-Attack 模型产生了 Adversarial Example， 使用这些example配合原有的数据集一起进行训练。

最终的实验效果上， 在Spider-Syn 数据集，目前的各个方法基本都下降了20个点以上， 作者使用的两个针对性增强方法分别也能提升10个点以上，可以说是非常的有效果了。 但离模型原本的效果也仍然有一定距离，说明这个方向仍然也有可以做的空间。

### LGESQL Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations 

Tag:图网络  Tag:Text2SQL Tag:语义解析

ACL 2021

https://aclanthology.org/2021.acl-long.198.pdf

把Text-to-SQL问题使用图网络建模已经有一些工作在做，但是这些工作基本都是使用以节点为中心的网络，并且使用不同的参数矩阵来对不同类型的边进行编码。 这样一方面就忽略了在边的拓扑结构中隐藏的语义信息，另一方面也不能对一个节点的 local 和 non-local的关系进行非常好的寻找。 在这篇工作提出了一个使用 line graph 来增强图建模的方法。 

方法的主要特点是提出了一个line graph, 和传统的 node graph相反， line graph 就是把经典图中的边看作节点，节点看作边。  这样形成的图其实就是之前的图的对偶图。 所以本文使用了两个几乎完全相同的网络来对line graph 和 node graph 进行建模。 最后再把对应的编码的拼接起来进行解码，解码的方式和rat-sql中的方法也完全相同，都是一个树形解码的方式。 同时本文还提出了一个gaph pruning 的辅助任务，用来识别正确的边，这也加强了网络对图的建模能力。

在实验效果上，作者在spider数据集上取得了 72%的效果，是目前最好的效果。 同时对比实验中也说明了line-graph引入带来的效果。

### Optimizing Deeper Transformers on Small Datasets 

Tag:Text2SQL Tag:语义解析 Tag:优化方法

ACL 2021

https://aclanthology.org/2021.acl-long.163.pdf

之前的工作大都普遍任务transformer需要大量的数据才能成功训练，在小的数据集上人们一般选择使用预训练好的模型进行微调。 但是这篇工作说明了Transformer通过更改一些初始化和优化的方式，可以在很小的数据集上也能训练的很好。 

之前已经有T-Fixup这篇文章做了相关的工作，但是T-fixup只针对了vanilla transformer，没有针对一些带有关系边的transformer,并且对于输入的初始化有严格的限制， 不能适配由预训练模型得到的初始化。 对此，本文做了一些改进， 首先本文通过一些数学理论分析了Transformer难以优化并且需要进行一些warmup  step的原因。 最后算法做的具体改进为1. 使用了Xavier初始化方法对于一些自由的参数。 2. 移除了warm-up 和所有后续transformer中的layer normalization(这被证明会影响优化的方差)。 3. 前向传播的时候控制每个向量中最大维度的大小 4. 通过上述的维度打来帮助相关 attention层进行参数的确定。 
最后作者在Text-to-SQL 数据集上进行了实验，使用优化后的RAT-SQL方法取得了70.9的效果，是当时的最佳效果。 并且在阅读理解任务上也取得了SOTA的效果。 

### From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding

Tag:语义解析

ACL2021

https://www.aclweb.org/anthology/2020.acl-main.608.pdf

本文提出了一个使用无监督的方式来做 semantic parsing的方法， 主要思想是，给一个输入的文本，该模型会同时生成范例文本和logic form,   并且在decode使用语法来限制了搜索的空间。  

本文的方法步骤大概是，首先选用一个预训练的 paraphrase generation model， 这个模型的主要作用是把文本变成规范化的文本， 但是目前预训练的模型其实做不到生成这种风格的文本。 所以本文首先写了一个从logic form 到规范化文本的一个标准解析器，产生一个数据集，可以使用这个数据集进行fine-tune。 然后使用fine-tune之后的模型生成文本的， 通过规则化解析也就生成了logic-form。
从实验结果上看，在OVERNIGHT 数据集上取得了SOTA的效果，在GEOGRANNO 和GEO 两个数据集上取得了远远比其他无监督方法好的效果。

### Span-based Semantic Parsing for Compositional Generalization

Tag:语义解析

ACL 2021

https://aclanthology.org/2021.acl-long.74.pdf

使用 seq2seq 方法的 semantic parsing 被认为在组合泛化的效果上的比较差。 本文主要去解决这个问题。具体的， 本文使用了一个span-parser， 即使用使用句子中的每个span都去预测sub_tree(可能是实际值也可能是中间节点)。  这样就提高了模型学习文本和logic form之间关系的能力， 可解释性也就提升了， 理论上也就能提升组合泛化性。  文章中尝试了完全监督的模型（具体到span-level的标注）， 和隐式监督（只有针对整个句子的监督），这种监督需要EM算法来优化。  同时在inference的时候，使用了一个基于CKY的推理算法，能够保证生成语法正确的树。 

实验结果： 对 GEOQUERY, CLOSURE, SCAN 三个数据集进行了实验，在独立同分布的train-test 集划分设定下，和之前的seq2seq方法效果类似，但是在针对组合泛化性的 train-test 集划分设定下效果有了非常明显的提升。

### Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both? 

Tag:语义解析

ACL 2021

https://aclanthology.org/2021.acl-long.75.pdf

本文对 semantic parsing 领域目前的主要挑战进行了分析，主要分成了两个方面，分别是自然语言的多样性和组合泛化性，目前的很多模型都很难同时做好这两个任务， 比如 seq2seq模型可以很好的利用神经网络的特性建模自然语言的多样性， 一些基于语法的模型可以相对比较好的处理组合泛化性但是对于一些特定的语言输入却无法产生输出， 本文就去探讨如何同时解决组合泛化性和自然语言多样性的问题。

本文提出了一个NQG-T5算法，其实就是简单的结合了NQG和T5两种算法，NQG是一种基于QCFG文法的parsing系统，这也是本文的主要贡献了，根据语法特征通过一些CRF模型和MML目标进行优化，但是这个模型对一些多样性的自然语言输入是无法处理的。 如果遇到无法处理的文本，就直接使用 T5 这个预训练模型进行seq2seq的输出。 

在实验结果上，在SCAN这个生成数据集上取得了最优效果，在GEOQUERY这个简单的数据集上取得了最优化效果，但是仍然在SPIDER数据集上效果不好。甚至NQG直接准确率是0。  总体来看，本文的工作还是相对比较简单， 没有把两个组件结合到一起，后续这个方向应该有非常大的空间。

### On the Sentence Embeddings from Pre-trained Language Models 

Tag:预训练模型

EMNLP 2020

https://aclanthology.org/2020.emnlp-main.733.pdf

尽管如BERT这样的预训练模型在NLP任务上取得了很大的成功，但是不经过 fine-tune 的BERT模型在判断句子相似性这个任务上做的并不算好。 这篇文章对这个问题进行了一些偏理论的分析，并且提出了改进方法成功提升了模型的性能。

作者首先对于BERT使用mask language model的预训练方式进行了一些理论上的分析，主要有了两个发现 1. 词出现的频率很大程度上的影响了编码空间。 2.  高频词分布的非常集中，低频词相对是比较分散的。   针对上面两个问题，作者提出了一个 flow-based方法，主要是通过一个可逆的变换把BERT变到一个标准高斯分布中，这样编码就从各向异性变成了各向同性。

在实验结果上，在SST的7个数据集上都取得了比较大的提升效果。 并且还详细分析了相关性和编辑距离的关系，说明了学到了有效的距离。

### All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text 

ACL 2021  Outstanding paper

https://aclanthology.org/2021.acl-long.565.pdf

在自然语言生成领域，人类评估一般被认为是标准， 但是随着模型生成的文本质量的提升，人类评估者是否能够非常好的对文本进行评估是需要进一步讨论的。 所以本文通过设计实验来评估人类是否能够区分 人类写作的文本和机器生成的文本， 在故事，新闻和食谱三个领域中，实验结果表明对于GPT2生成的文本识人类别准确率为57%， 对于GPT3生成的文本人类识别准确率为49%（甚至不如随机的50%）。 作者分析主要是随着模型性能的提升，生成文本的流利程度增加，错误往往发生在细节和逻辑层面，在生成的文本很长的情况下，非领域专家相对粗浅的阅读很难进行区分。 本文后续又通过三种方式对评估人员进行训练，分别是提供指导意见，给一些例子，给出对比，发现在给出例子的情况下准确率能到55%左右，虽然不是特别高，但是也已经好过了随机效果。   所以基于上述的实验结果，作者推荐后续的NLG领域在进行人工评估的时候最好给评估人员例子进行训练，并且对实验的设定进行更细节的报告来提升实验的可信程度。

### KILT a Benchmark for Knowledge Intensive Language Tasks

Tag:外部知识引入 

https://arxiv.org/pdf/2009.02252.pdf

目前的很多NLP任务都需要从外部的知识库中寻找一些知识，然后辅助任务的完成。 但是目前这类任务有非常多的数据集，每个任务都有不同的知识库，不同的假设，需要不同的数据加载器，评估方式和分析方式。  这样同一个模型在测试的时候就会带来不必要的计算开销，并且也很难测试知识在不同领域的迁移。

 所以本文提出了一个统一的知识库，KILT，使用了一个统一的知识库，就是wiki 百科的2019/08/01的版本，针对其他数据集中的不同版本，首先对问题的链接进行了重新匹配，对于文字修改的部分，取BLEU最大的部分进行匹配。  同时，本文针对Fack-checking, entity-linking,slot-filling,question answering 等任务的输入和输出都进行了适配，所有KILT框架可以适配于这些任务。 最终整个模型使用了一个简单的 seq2seq encoder的baseline,就能在上述各个任务上取得非常有竞争力的实验结果。

感觉这篇文章的主要贡献应该还是工程上的，统一了wiki百科的版本，给了一套统一的输入输出实验代码，测试起来更容易并且更有说服力了？

### WIKITABLET A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections 

Tag:数据集

https://arxiv-download.xixiaoyao.cn/pdf/2012.14919.pdf

2021 ACL findings

这篇文章主要在做 Data-to-Text 的数据集，之前的 Data-to-Text 数据集要么是多领域的单句生成，要么是单领域长篇生成。 这篇文章提出了一个非常大型的数据集，把wikipedia的文本和对应的表格数据和元数据进行了对应。 

总体来说，这个数据集有两大挑战， 一是在一些data-to-text 的case 上需要一些world knowledge, 有相关能力的模型可以在这里测试。  二是包括了多种多样的表格类型和数据领域。 

作者采用了一些比较简单的方法进行了测试，主要是使用了Transformer模型，并且辅助了一些优化方法，实验效果看起来不是很好，Transformer large模型甚至还不如Transformer base模型。  作者还进行了一些人工检查，结论是生成的文本流畅程度和质量不错，但是出现了一些一致性和事实性问题。  看文章中的说法这应该是非常大型的数据集，质量比较高，还有非常高的提升空间。

### Describing a Knowledge Base

Tag:数据集

ACL 2018

 https://arxiv.org/pdf/1809.01797.pdf

本文也是做 data-to-text 生成的问题。 本文主要有三个贡献， 一是提出了一个使用 slot-aware attention， table position self-attention  的pointer network 来做 data-to-text问题。 二是提出了一个新的评价指标KB reconstruction 。 三是提出了一个新的数据集。  下面一个一个的说。

方法方面，由于data-text 要求准确生成表格中内容，之前的seq2seq方法难做到，只使用 pointer network 又很难把slot-type和slot-value进行对齐，所以作者提出了slot-aware attention来解决这个问题。 同时，一些表格中的slot相互之间是有关系的，之前的模型在生成的时候可能不会考虑到，又增加了Table position attention 
来解决这个问题。

评价指标方面，提出了KB reconstruction指标，因为之前到BLEU指标很难全面的评价生成文本的质量。KB reconstruction的主要思想是根据生成的文本重新生成KB，和之前的KB逐项对比，生成生成一个准确率。  但是最重要的问题，根据文章的描述，这个生成KB好像是人工的！！

数据集方面，使用Wikipedia (2018/04/01) 和 Wikidata (2018/04/12) 在person 和 animal 两个领域进行了对齐，最后数据量是106,216 。 

实验结果上，BLEU最好也就能达到23，不算很高，倒是采用KB reconstruction能达到70%以上的F1 score。

### GenWiki A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation 

Tag:数据集

https://aclanthology.org/2020.coling-main.217.pdf

ICCL 2020

对于knowledge graph-text 领域的数据收集是十分困难的，所以当前的很多工作都在非常小的数据集上学习，并且该领域的无监督学习方法也开始活跃起来。 然而即使是采用无监督学习的方法，也有数据量不足的问题， 因为一个合格的无监督knowledge graph-text 数据需要满足四个条件： 1.  knowledge graph和text 的分布要尽可能相同 2. 文本要包含高质量的 entity 标注 3. 要比有监督数据集的规模大很多 4. 要有人工标注的测试集。 想要同时达到上面的条件还是非常困难的。 所以本文提出了GenWiki数据集，包含了1.3M的无监督数据对和1k的标注测试集。 

无监督数据的构造， 爬取了wikipedia的文本，并且对于网站上出现的所有包含超链接的实体查询相关的知识图谱， 之后进行一些过滤，去掉明显不相关的文本和知识图谱元素，然后设计了一些规则和算法对文本中的实体进行标注。 这样形成的文本和知识图谱对就构成了训练集。  对上述形成的无监督数据对，用户需要判断是否容易修改，如果容易修改，就对其进行修改然后形成正确的测试集。

作者尝试了多种无监督方法，最招的CycleGT 的BLEU值能达到40%以上， 效果已经还不错了，错误分析也是常识性错误比较多一点。

### WikiGraphs A Wikipedia Text Knowledge Graph Paired Dataset 

Tag:数据集

https://aclanthology.org/2021.textgraphs-1.7.pdf

这篇文章提出了一个Wipipedia 文章和知识图谱对齐的数据集， WikiGrpah, 可以方便条件文本生成，知识图谱生成，图表示学习等领域的研究。  WikiGrpah 数据集中的每条数据是wikipedia 文章和free-base中的子图对。  这个数据集的主要特点是文本比较长，知识图谱也比较大。 但是数据没有上一篇 GenWiki多，只有23522个训练数据，和48个验证集数据，43个测试集数据。 （验证集和测试集也太少了）

数据集的构造过程，先找到 WikiText-103中的文本，根据标题这样的关键信息去匹配Freebase中的实体，如果能匹配上，就进行下一个步骤，下一个步骤是去根据上一步匹配上的核心实体，在知识图谱上保留所有1-hop的子图。  最后一个步骤是过滤数据，对于相同类型边只选取一个典型。

在 data-text, graph-retrieval，text-retrieval 三个任务上都进行了实验，data-text 目前最好的效果能达到BLEU值30左右， text-retrieval  recall@5 能有35， graph-retrieval能达到100% （作者解释是这个任务比较简单）。

### Text-to-Table A New Way of Information Extraction 

Tag:Table2Text 

https://arxiv.org/pdf/2109.02707.pdf

这篇文章提出了 text-to-table这个任务，可以认为是 table-text 任务的反向任务。 作者认为，这个任务和其他的信息抽取任务有两点不同，1是数据规模，可以从非常长的文本中抽取很大的表格。 2是这个抽取是完全数据驱动的，不需要显式的定义schema。 

因为schema不需要显式定义，table的结构约束并不多，所以本文采用了一个 Seq2Seq的框架来解决这个问题。 具体来说,baseline模型采用了BART这个预训练模型。  尽管table的结构限制并不多，但仅仅使用seq2seq模型仍并不能保证生成结构的准确性， 所以本文又额外增加了两个策略来缓解这个问题。

第一个策略是table constraint, 由于seq2seq模型不能保证生成的数据每行数量一样多，所以设计了这个算法首先记下第一行的长度，之后每行decode产生这个长度时就自动开始decode下一行。  第二个策略是table relation embedding,   由于table数据本身不同cell之间是存在关系的，比如一个cell 和其row header和column header都有相关性，所以在生成每个cell的时候都增加了和其相关cell的attention。  具体来说，采用思路就是Self-Attention with Relative Position Representations 这篇文章中的Relation-aware Self-Attention（但这篇文章没引用）， 即在 transformer中增加了关系编码，如果两个编码之间之间本身有关系（cell 和 header关系），在计算attention的时候会增加一个关系向量。

在实验结果方面，在Rotowire, E2E, Wikitabletext ，WikiBio 数据集上进行了实验，比较的方法基本是使用RE抽取关系，然后再构成表格。 评价指标就是和标准表对比，如果一个cell和值，column header, row header都一致，就算正确，然后计算  Pre, Rec, F1。 从结果上看，本文使用的改进Seq2Seq方法在其中三个数据集取得了最好的F1值。 但比Vanilla Seq2Seq方法的提升并不多。 

### BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension 

Tag:预训练模型

https://aclanthology.org/2020.acl-main.703.pdf

ACL 2020

从目前来看已经是一个非常出名的预训练模型了，之前一直没有对这篇文章进行总结。 

BART 是一个使用去噪自动编码器进行预训练的seq2seq模型。 在BART之前已经有很多的预训练模型采用了 MASK-LANGUAGE 方法来进行预训练，但是这些模型着重的END TASK都过于局限？  所以这篇文章提出了BART，和BERT不同的是，BART是一个seq2seq模型，同时使用了transformer的encoder和decoder，这也使得bart对一些生成式的下游效果更好。

具体来说，bart的预训练方式，首先对文本进行打乱，打乱的方式有以下几种，token masking(和Bert中相同)， Token Deletion （把一些词删除）， Text Infilling 
（根据泊松分布采样出span长度，mask这些span）, Sentence Permutation （对句子进行完全打乱）， Document Rotation（对文档进行旋转）， 把打乱后的文本放入encoder, 把正常顺序的文本作为decoder的输入，来构建出完整的文本。 

最终在 Sequence Classification Tasks ， Token Classification Tasks ， Sequence Generation Tasks ， Machine Translation 上都稳定取得了最好效果。

### Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 

Tag:预训练模型

https://arxiv.org/pdf/1910.10683.pdf

Journal of Machine Learning Research 21 (2020) 

著名的T5模型，T5是Text-to-Text Transfer Transformer的缩写。 主要的思想是把所有的nlp任务都建模成了一个 text-to-text 任务，使用了一个encoder-decoder的transformer架构来学习几乎所有任务， 取得了不错的效果。 本文的特点是进行了非常大量的实验

### Improving Language Understanding by Generative Pre-Training

Tag:预训练模型

https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

GPT 系列的第一篇论文， 最近相对预训练模型系列进行研究，回来读一下。

动机： 在GPT出现之前的大多数工作都是完全监督学习的，必须要有标注数据才能学习。 GPT这篇文章使用了大量的无监督数据来进行预训练，然后使用预训练后的模型在无监督数据上进行fine-tune得到好的结果。 GPT应该是最早提出了预训练的一批工作，并且是最早使用Transformer进行预训练的工作。

训练方式： 无监督语言建模任务，即在语料库中，给出上文预测下一个单词。 预训练时候就只采用这一个任务和对应的loss。 对具体的任务进行fine-tune上的时候，目标是具体任务的目标和语言建模loss。

模型组成：使用了12层的 transformer decoder。 一共有 117M 参数。

数据集： 使用了 BookCorpus 数据集，有7000本书的文本。

实验效果： 在12个任务中的9个任务中都取得了最优效果。

### Language Models are unsupervised multitask learners

Tag:预训练模型

https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

GPT2论文初看好像改动不是很大， 但总结下来有两点， 1是提供了一种新的思路，生成式的预训练模型本身可以直接去解决多种任务。 2. 模型的规模大大提升之后取得了非常优异的效果。


新的理念： 之前的大多数任务都会被建模成 P(output|input) ， 但 GPT2的目标是使用同一个无监督的模型去学习多个任务， 具体来讲模型的输出就会变成 P(output|input, task)。  给模型同时输入任务描述（条件），这样就可以针对相同的输入产生不同的输出。 这种建模也是能进行 zero-shot learning 的根本。 

理念的实现方式： 在输入中加入不同的prompt可以达成这样的效果。 比如(translate to french, english text, french text)这样的文本串中，输入translate to french, english text 让模型输出后面的 french text就完成翻译的任务。  刚看到这里的很困惑，为什么GPT2可以清楚的解释不同的prompt并且产生输出？ 可能的解释是，GPT2的模型性能非常强，有语义的prompt可以给GPT2非常强的提示，并且即使这样zero-shot效果也不是特别好的，还是需要给几个例子，即few-shot learning 能产生更好的效果。

数据集： 在 40G 的相对比较高质量的 WebText 数据集上进行了预训练。

模型： 用了48 层的 transformer decoder, 50,257 词表大小， 512的batch_size,  最终参数量是 1.5B。

实验效果： 在zero-shot setting下，在8个语言建模中的7个取得了最好效果。 在阅读理解，翻译，摘要等任务的zero-setting也取得了还不错的效果，但是没达到SOTA。

### Language models are few shot learners

Tag:预训练模型

https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

Nip2020 最佳论文

GPT3模型仍然是相对于GPT2模型的一个增量改进， 其实并没有新的理念提出，但是由于模型的性能更加强大了，也由此让人们对于GPT2提出的理念理解更加深刻了？

理念更新： In context learning,  文章中说，在训练时任务是给context word预测下一个词，但模型在这个过程中也会对于文本的模式进行自动的学习，来更好的对下一个词进行预测。 这样就引出了一种新的模式： 不需要梯度更新的few-shot learning。 直接把few-shot 的case作为提示放在文本前面（比较长的prompt）,GPT3会自动学习这种模型然后产生对应的输出。  由于GPT3非常强大的模型性能和prompt多样性，GPT3真实现了一个效果还不错的 zero-shot learner。  

数据集： 在Common Crawl, WebText2, Books1, Books2 and Wikipedia 5个数据集上进行了预训练。

模型： 使用了96层的 transformer decoder，每个decoder有96个attention heads。  最终有  175 billion 参数。 

实验结果： 在 语言建模任务上， 在 zero-shot setting 下就能超过SOTA，在非常多其他的任务上比如翻译，问答也能使用  zero-shot setting 或者  one-shot setting  达到最优效果或者接近最优效果。

### Its Not Just Size That Matters Small Language Models Are Also Few-Shot Learners

Tag:预训练模型

https://aclanthology.org/2021.naacl-main.185.pdf

NAACL 2021

GPT3 模型在很多任务的 few-shot learning 设定上取得了非常出色的效果， 但是GPT3需要的参数量十分巨大，一般的研究者很难去使用。 这篇文章提出了一个使用cloze question 配合梯度更新的方法， 在只有 GPT参数量 0.1% 的情况下在一些任务上取得了比GPT3更好的效果。 

先介绍本文的前序工作， PET（pattern exploiting training）。 PET把很多NLP任务建模成了以下步骤：
1.  通过一个 pattern P 把 输入文本 X 变成 T*, T* 中有 cloze question ，包含一个mask。 
2. 通过 预训练语言模型 预测其中的mask, 产生输出 Y。 
3. 使用一个 verbalizer V 把Y映射到T，其中 T 是该NLP任务的特定符号，比如情感分析的两个类别。 
这样的一个 pattern-verbalizer pairs 就是 PVPs。
同时PET中还使用了多个 PVP，使其相互学习，从无监督数据中增强了模型的效果（有点类似self-training）。

本文在PET上做了一点改进，之前的PET输出只能是一个token, 不能满足多种NLP任务的需要，其实就很类似 seq 的生成了，先预测第一个token,取概率最大再去预测下一个token。 作者分成 inference 和 train 来去介绍，很类似seq2seq learning的基本 setting。

实验： 在QA任务， Text entailment, 问答的多个数据集上做了实验，在这些数据集大都能取得比GPT3更好的效果。

### KnowPrompt Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction 

Tag:Prompt

https://arxiv.org/pdf/2104.07650.pdf

近期，prompt-tuning 方法在 few-shot setting 下取得了不错的进展，但是在关系抽取领域中，如何更好的设计prompt仍然是一个需要领域专家的工作。 本文提出了一个将知识引入 prompt-tuning 的方法来去解决关系抽取问题。

方法，首先还是按照经典的 prompt 方式转换输入，把句子变成 X  E1 【mask】 E2, 这种形式， X是输入的句子，E1，E2是实体，【mask】是要预测的关系，之前的prompt方法是把mask预测出的词和目标label做一个一对一映射，这样做就没有用到关系标签的丰富语义。 于是本文做了如下的改变
1. 在对mask进行预测的时候，在输出层中，把输入的维度进行扩展，维度从词表的大小扩展到词表的大小+关系的数量。 直接看输出结果在后面 关系数量大小的维度上 logit 来判断类别。这样mask language 的loss就可以直接作为一个交叉熵。
2. 在实体前后加入特殊符号 [sub] 和 [obj], 使用实体类型对应的向量来进行初始化。  把关系向量使用其中包含单词的向量的来初始化。 然后设置了一个 KE loss， 就是把构成三元组的实体 |h+r -t|尽量小，再负采用一些数据，这些数据的 |h+r -t|尽量大， 有点对比学习的意思。
实验效果，在5个数据集的标准设定和低资源设定下都取得了不错的效果。

### Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference 

Tag:Prompt

EACL 2021

https://aclanthology.org/2021.eacl-main.20.pdf

有一些工作通过给预训练模型一些任务描述来无监督的解决这些问题，但是这种方法一般是比相对应的监督学习方法差的。 本文就提出了一种Pattern- 
Exploiting Training (PET) 方法，是一种半监督学习方法，通过把问题建模成完形填空来增强模型对于问题本身的理解。

PET把很多NLP任务建模成了以下步骤：
1.  通过一个 pattern P 把 输入文本 X 变成 T*, T* 中有 cloze question ，包含一个mask。 
2. 通过 预训练语言模型 预测其中的mask, 产生输出 Y。 
3. 使用一个 verbalizer V 把Y映射到T，其中 T 是该NLP任务的特定符号，比如情感分析的两个类别。 
这样的一个 pattern-verbalizer pairs 就是 PVPs。
同时PET中还使用了多个 PVP，使其相互学习，从无监督数据中增强了模型的效果（有点类似self-training）。

本文在 Yelp， AG’s News ， Yahoo ，MNLI 的 few-shot setting 下都取得了不错的效果。

### Bidirectional Transition-Based Dependency Parsing 

Tag:机器翻译

https://ojs.aaai.org//index.php/AAAI/article/view/4733

AAAI-2019

Transition-Based Dependency Parsing:   把parsing tree变成一系列的action(transition),   也就是这个任务需要学习文本到action sequence的映射，其实和seq2seq任务很像了。 action一般有三种动作，SHIFT： 把输入队列中的元素移动到栈中。 LEFT（reduce）： 栈顶的两个元素是左子树关系。 RIGHT(reduce)： 栈顶的两个元素是右子树关系。

之前的很多 Transition-Based Dependency Parsing都是从左到右进行处理，这样容易造成误差累积，这篇文章同时学习了从左到右生成和从右到左生成两个parser, 然后设计了三种不同的decode算法来解决这个问题。

Vanilla Joint Scoring:  最简单的decode方法，先分别用两个parser完整的生成，然后分别使用两个paser对生成的结果进行打分，选择打分相加最高的。

Joint Decoding with Dual Decomposition:  其实仍然是一种基于 score的思想，优化目标是两个方向的parser 生成的 tree得分加起来最高, 限制条件是两个 tree 相同， 这是一个有限制条件的优化目标，作者使用拉格朗日松弛找到一个可以优化的上界（我能简单理解拉格朗日松弛，但是不理解他的公式怎么产生的）。  然后针对这个优化目标设计了一个decode的算法，就是Joint Decoding with Dual Decomposition， 主要思想是迭代法，每轮迭代两个paser会产生两个矩阵（和tree一样）， 如果两个矩阵不同就把不同的部分单独拿出来称为矩阵u，在后续迭代的时候输入矩阵u，使得两个paser得到的解尽可能相似。 因为由于之前说的这个算法能优化上界，所以也能尽可能的获得一个比较优的解。

Joint Decoding Guided by Dynamic Oracle:  这个decoding方法用到了Dynamic Oracle，这是一种在测试阶段提供一个gold树来防止误差累积的方法（Training Deterministic Parsers with Non-Deterministic Oracles）。 本文的这个decoding算法就用到了 Dynamic Oracle， 仍然是迭代的思想，两个paser在上一轮生成的树互为gold tree输入到下一轮，使用 Dynamic Oracle来指导两个paser生成相同的 tree。

最终实验结果上，在很多数据集都有提升，但提升也都不到一个点的样子。

### Constrained Language Models Yield Few-Shot Semantic Parsers 

Tag:语义解析

https://arxiv.org/pdf/1902.09492.pdf

这篇文章探索了 大规模预训练模型能否在 semantic parsing 任务上做  Few-Shot  learning， 由于semantic parsing 任务生成的都是结构化表示，但预训练模型生成的都是自然语言，所以本文首先建立出了一种可控标准的英文表达形式（这种形式和结构化表示是一一对应的），可以通过规则化的代码相互转化。  然后控制预训练模型去生成这种可控的标准英文表达形式。  

本文的方法主要有两个点，1是Dynamic Prompt Creation， 其实就是用example作为prompt, 使用了GPT3动态选取了example。 2. 是Constrained Decoding， 每一步decode并不是从全词表生成，而是设置了一个validNextTokens 函数，这个函数来判断可以生成的下一个token范围，来确保生成的句子是满足固定格式的。 本文没有在GPT3上没有fine-tune, 在一些其他相对比较小的模型上也使用了一些方法进行fine-tune。

实验结果，在Overnight,Break , SMCalFlow 数据集上都在few-shot setting下进行了实验， 在overnight数据集上能取得和全量数据类似的结果，在其他两个数据集上离全量数据集SOTA方法差距还比较大。

### HTLM: Hyper-Text Pre-Training and Prompting of Language Models 

Tag:预训练模型

https://arxiv.org/pdf/2107.06955.pdf

使用大量爬取的html数据预训练得到的模型， 使用了类似BART的预训练方法，用HTML里自带的prompt达成里一个效果还不错的zero-shot summarization， 还可以进行一些HTML补全等任务。

### Pre-train Prompt and Predict A Systematic Survey of Prompting Methods in Natural Language Processing 

Tag:Prompt

https://arxiv.org/pdf/2107.13586

一篇关于 prompt-learning 的综述文章。

本文首先梳理了NLP的发展历程，第一阶段是手动构建特征让模型进行学习， 可以总结为 feature engineering ， 第二阶段引入了深度神经网络，模型可以自己学习特征，但是仍然需要对模型架构进行设计，称为architecture engineering。  第三阶段监督学习的方式遇到了瓶颈，形成了新的范式是 pre-train and fine-tune paradigm， 在这个阶段需要去做的是objective engineering，即需要去设置fine-tune的目标函数。 在最近出现了一个更新的小阶段， 可以被称为pre-train, prompt, and predict ， 即首先预训练模型，然后设计prompt，之后去预测。 这个阶段强调的就是 prompt engineering 。

第二个部分作者对prompt的基本概念进行介绍，把使用prompt的方法总结成了三个步骤。 第一个步骤是 Prompt Addition,  即对输入X使用一个模版，模版中包含了一些slot。 如果slot在中间，任务就被称为 cloze prompt(完形填空)， 如果prompt在最后就是 prefix prompt。  并且值得注意的是模版word不一定是自然语言词，也可能是连续向量。  第二个步骤是 Answer Search, 即找到一个词填充上 prompt 使得LM的得分最高。 一般会预先定义一个答案集合，如果是分类任务，会选出特定的词来对应类别，如果是生成任务，可能是全词表。  第三个步骤是Answer Mapping 
， 生成的词是词表空间的表示，需要mapping到label到空间。  之后作者分成了几个方面去讲解了prompting方法需要注意的问题，分别是 pre-train model choice,  prompt engineering, answer engineering, 	 Expanding the Paradigm,  Prompt-based Training Strategies。  
Prompt engineering： 这一章节列举了一些产生prompt的方式。  第一种是Manual Template Engineering，即人工设计模版。  但人工很难设计出最优的 prompt。 第二种是Automated Template Learning， Automated Template Learning 是目前研究的主流了，可以从多个视角进行分析， 首先可以分成离散的prompt和连续的prompt 设计，还可以分成静态和动态（即对于不同的输入有不同的prompt）。  对于离散的 prompt,  几种相对主流的方法分别是1.  Prompt Mining,  这是一种数据挖掘的方式，在很大的数据集（比如 wikipedia） 中找到x和y的最佳依赖路径。 2. Prompt Paraphrasing， 这种方法的原理是先找到一个人工设计的prompt，然后用一系列自动的方法对其进行改写，比如back-translation 等等，从这当中找出最好的一个 prompt。3. Gradient-based Search ， 对于给定的预测， 使得模型去优化prompt的词选择，通过一种迭代的方式最终选出prompt。  4. Prompt Generation  ,  有两篇相关的工作是使用 T5 这样的生成式预训练模型来生成模版，感觉是类似于定义了一个模版的模版，定义了在哪里插入输入数据，然后使用预训练模型来生成模版。   5. Prompt Scoring ，  一种给 prompt打分的方法，首先给 人工设计出一些prompt， 然后使用语言模型来对这些 prompt进行打分。  然后是 Continuous Prompts， 其实 prompt并没有必要必须要使用人类能能够读懂的语言，只要是能辅助语言模型完成具体的任务就可以， 所以可以直接建模 prompt 为编码空间中的向量。 这样做带来了两个好处， 第一个是取消了模版的单词必须是自然语言这个设定。 二是模版的编码和语言模型的参数无关，由自己的参数进行驱动，有自己的参数，可以使用下游数据进行fine-tune。    Continuous Prompts 的方法可以分成下面几类， 1. Prefix Tuning， 在输入的 x 前加入一些连续的 prompt (个人理解是正常的token id需要经过embedding层变成向量然后经过transformer编码， 而连续的prompt需要直接给一个 embedding，不需要transformer的embedding层了)， 把预训练模型的参数冻结，只去调prompt的embedding。  2.  Tuning Initialized with Discrete Prompts， 使用已经搜索好的离散 prompt进行初始化，然后再进行优化。   3. Hard-Soft Prompt Hybrid Tuning, 把hard prompt 和 soft prompt结合起来进行优化，P-tuning 在learnable prompt template 中增加了一点 anchor word 提升了效果。

Answer Engineering:  首先是 answer 的形式，一般来说有三种 token, span(multi-token), sentence, 一般 token, span会用在分类任务上， sentence会用在生成任务上。  比较关键的是从词空间到label的映射过程，即Answer Space Design Methods， 有三种方法。 1.  Manual Design ， 一般生成任务都是 identity mapping， 在全词表上生成，没有限制。  2. 很多分类任务会对输出的词表进行限制，把相关的词和一些类别进行对应， 一般也是一一对应的。  2.  Discrete Answer Search   人工设计很难达到最优，所以有一些工作研究自动搜索mapping关系， 经典的离散搜索方法包括  Answer Paraphrasing （使用 back-translation 对答案进行各种改写，改写后的作为词表词）， Prune-then-Search （大概先匹配出可能的token集合，然后在数据集上去最大似然？）， Label Decomposition （把label拆成好多个词，这些词就是需要的token）。 Continuous Answer Search（也是类似于连续prompt， 使用一个连续的embedding作为answer词的表达）。

Multi-Prompt Learning: 使用多个 prompt的方法， prompt ensembling(对同一个任务同时采用多个 prompt进行集成学习)， Prompt augmentation （其实就是 few-shot learning 了， prompt就是一些该数据集的例子）。 Prompt Composition （把一个任务拆解成多个子prompt任务，然后再集合起来）， Prompt Composition （把一个比较长的 prompt拆解成子prompt分开去解决）
。
Training Strategies for Prompting Methods， 主要是参数的更新方法 1. Promptless Fine-tuning， 就是最经典的 pretrian-finetune。 2. Tuning-free Prompting （像GPT3那样的完全不需要调节任何参数）3. Fixed-LM Prompt Tuning （只调节 prompt相关的参数，不去调节语言模型的参数）3.  Fixed-prompt LM Tuning （固定prompt参数，去调节语言模型） 4. Prompt+LM Tuning （对于prompt和语言模型全都调节）。

Applications: 目前 prompt方法相关的应用，只列举几个比较关心的， semantic parsing(Constrained Language Models Yield Few-Shot Semantic Parsers)： 看成了一个 text-to-text的任务，默认了每个方法提供了validNextTokens，来限制decode的输出。  Text Generation（使用 prefix prompt对于text-generation还是比较自然的）。

几个和prompt比较相关的话题， Ensemble Learning （集成学习）， Few-shot Learning ， Larger-context Learning （在输入中增加更多的context信息）， Query Reformulation， QA-based Task Formulation， Controlled Generation ， Data Augmentation。

挑战：  1. prompt 设计，目前大多数应用还是在 分类和生成任务，对于信息抽取的等任务仍然很少有做。  使用prompt来生成结构化信息也很少有工作在做。 如何同时考虑 prompt和answer的设计也是一个比较大的挑战。  2. Answer Engineering, 分类任务的两个挑战是如何选取最优answer space和答案有多个词的时候如何去生成（X-FACTR: Multilingual factual knowledge retrieval from pretrained language models. ）。 生成任务的挑战，多reference的学习如何设计？  

### NumER A Fine-Grained Numeral Entity Recognition Dataset 

Tag:数据集

https://link.springer.com/content/pdf/10.1007%2F978-3-030-80599-9.pdf

NLDB 2021

数字实体可以文本中发挥非常重要的作用， 数字实体可以暗示出名字，长度，实体等概念，但是在之前的工作中，一般都还是着重于单词实体的，本文就提出了一个数字实体识别数据集，包括了新闻，wiki文章，问题和一些指示。 并且训练了一个数字BERT模型来去检测文本中的数字实体，最终达到了95%的准确率。 

本文的标注上，主要是分成了8个类别，标注的数据来源为1. 一些 text-to-sql的数据集  2. wikidata数据集 3. Epicurious 食谱数据集 4. 新闻分类。 

最终也仅仅采用了一些比较基础的baseline去实验，发现仅仅用预训练模型效果就已经不错了。

### Asynchronous Bidirectional Decoding for Neural Machine Translation 

Tag:机器翻译

https://arxiv.org/pdf/1801.05122.pdf

AAAI 2018

尽管seq2seq方法采用的attention机制目前取得了还不错的效果，但是仍然存在的问题是不能利用 reverse context, 即right-to-left 方向的 context,  这样就会有 left-to-right方向的误差累积问题。

于是本文，在训练的时候采用的模型包括一个encder和两个decoder，两个decoder分别是 backward decoder（从right-left 方向decode）和 forward decoder（从left-right)方向进行decode, 具体的流程是，经过encoder之后，先经过一个 backward decoder, 生成反向的表示，然后经过 forward decoder, 此时计算attention会同时计算 encoder中的状态和backward decoder中的状态，这样就考虑了到了reverse context， 一定程度上的避免了只有left-to-right方向的误差累积问题。

在几个数据集上都能取得一个点以上的提升，效果还是不错的。

### Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation

Tag:机器翻译

https://www.ijcai.org/Proceedings/16/Papers/392.pdf

IJCAI 2016

本文的主要目标是去优化在 seqseq 生成时 attention 矩阵的质量， 具体来说，之前的seq2seq模型在捕捉attention时可能仅仅会捕捉到特定方面的，甚至可能会有噪声出现（不正确的attention）。  于是本文设计了一种优化 attention 矩阵的质量 的方法， 主要方法是让 source-target 和 target-to-source 两个模型真对相同训练数据的attention矩阵算一个agreement 值，把这个agreement值也作为一个优化目标，最终使得两个模型的attention矩阵尽可能相似，最终优化出一个不错的attention矩阵。

本文设计了三种算法来去计算 aggrement 值， 分别是Square of addition (SOA)， Square of subtraction (SOS)， Multiplication (MUL) 。

实验结果上，提升的幅度还是不算小，说明了结果的有效性。

### Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing 

Tag:Text2SQL

EMNLP 2021， 但是目前阅读的版本应该和最终版本是有差距的，目前应该读不到最终版本。

https://arxiv.org/abs/2103.02227

cross domain text-to-sql目前主要面临了两大问题，1是测试数据库在训练时无法见到，这样就需要模型有很强的泛化性。 2是数据量比较小，并且标注数据比较困难。 

本文采用数据增强的思路来解决这个问题，即直接从数据库中生成数据，并且采用了一种SQL-to-question 的方法获取 sql-text pair。 具体来说，根据SQL的语法采样的语法模版，应用到各个数据库中，就产生了大量的SQL语句，然后采用了一个 Hierarchical Generation 的方法，把SQL和文本的对应部分划分出来，分别训练seq2seq模型，最终再把生成的数据拼接起来形成数据对。 

实验方面，在WikiSQL， Spider ， DuSQL 三个数据上的 seen 和 unseen两个设定上进行了实验，发现了都是有提升的。  比较有意思的实验是三种训练策略，使用 pre-train方法竟然是提升最小的。

### Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization 

Tag:Text2SQL

EMNLP 2021 短文

https://arxiv.org/pdf/2109.05157.pdf

本文属于一篇独辟蹊径的角度，虽然spider数据集确实是声明模型不需要领域知识的引入就能取得比较好的效果，但是本文却总结了5类需要领域知识的例子。 主要原因在于测试集和验证集的domain是不同的，一些隐含的领域知识可能需要对这个领域的学习才能得到。 于是作者在验证集中挑选出了一些需要domain kownledge 的数据作为新的spider-DK 验证集。

最终在实验结果上也表明，在新的验证集上，模型的表现并不好，这也就说明了目前的模型并不能够很好的建模领域知识。 同时一个实验数据也非常有意思，模型倾向于把预测order的顺序反向， 这样主要是由于训练数据label不均匀，所以后续解决这个问题可能也会成为一个方向。

### Natural SQL: Making SQL Easier to Infer from Natural Language Specifications 

Tag:Text2SQL

EMNLP 2021 findings

https://arxiv.org/pdf/2109.05153.pdf

本文提出了一种更好的SQL 表示， 降低了自然语言和SQL语言之间的GAP，让模型可以更好的进行训练和推断。  

本文提出的表示主要有三个特点： 1.  消除了GROUP BY， HAVING， FROM， JOIN ON这些语句，仅仅保留了 SELECT，WHERE，ORDER BY 2. 消除了 SET， UNION， EXCEPT. 等语句， 并且消除了嵌套语句。3  减少了需要的 schema 数量，使得schema-linking 更加简单。

同时由于本文的value只存在于where语句中，可以限制按照顺序生成，所以这种表示方法容易生成比较高执行准确率的模型。 最终效果，在执行准确率和exact match 都有提升，尤其在执行准确率上取得了SOTA效果。

### TinyBERT Distilling BERT for Natural Language Understanding

Tag:预训练模型

https://aclanthology.org/2020.findings-emnlp.372.pdf

EMNLP 2020

预训练模型对于提升NLP模型的性能起到了非常巨大的作用，但是预训练模型的计算开销特别大，在一些计算资源有限的设备很难运行。 所以本文使用 知识蒸馏（knowledge distillation）的方法使得一个参数量更小的BERT模型可以取得和标准参数量的BERT的类似效果。 

具体来说，设计了一个 Transformer Distillation 机制，专门针对transformer模型的知识蒸馏方法。  每个层的具体知识蒸馏方法稍有不同。  在做法上，分成了General Distillation 和 Task-specific Distillation，General Distillation 就是在大规模语料库上直接使用了 Transformer Distillation。 Task-specific Distillation 同时使用了  Transformer Distillation和数据增强， 数据增强的具体方法，随机mask输入中的词，使用标准bert还原。 

最终的实验结果，在只有bert10%左右参数下，取得了96%的效果。 并且对比实验说明，好像数据增强方法是这当中最有效的。



### UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING

Tag:半监督学习

https://arxiv.org/pdf/1904.12848.pdf

NeurIPS 2020 

本文研究了数据增强框架如何和半监督学习一起去使用，之前的数据增强方法只能用在监督学习上，而且普遍被认为作用相对比较小，本文探索图和结合大规模无监督数据和数据增强方法。

具体的方法其实非常简单，整体的loss分成了两个部分，一个是标准数据部分对应的 Supervised loss, 另一个是 Unlabeled Data经过模型预测产生label,然后把unlabeled data 经过数据增强， 这样就产生了 （x, y）, (x1, y) 两个pair,尽管y可能是有噪声的，但是只要让模型认为x和x1对应的label是相同的就可以了，这样经过一个 KL 散度就产生了Unsupervised Consistency Loss。 具体的数据增强方法采用的是相对比较高级的 Back translation, randAugment, TF-IDF word replacement (目标是保存信息含量高的单词，删除掉信息含量比较低的单词) 等。  本文同时还提供了一些理论分析（不愧是 NeurIPS论文）。

在多个领域的多个数据集都取得了不错的结果。

### GRAPPA GRAMMAR-AUGMENTED PRE-TRAINING FOR TABLE SEMANTIC PARSING

Tag:预训练模型  Tag:语义解析

https://arxiv.org/pdf/2009.13845.pdf

ICLR 2021

相对看过比较早一篇文章，再来总结一下。

主要动机，想要进一步的去fine-tune当前的预训练模型， 因为经典的预训练模型在处理文本和表格数据时还是存在一定的gap的。

主要做法，先使用一个 SCFG 从数据库中采样出SQL语句（作者也承认这样采样出的SQL语句其实很粗糙，可能需要进一步的筛选）， 然后构建了SQL和文本之间的对齐模版，生成SQL后可以直接生成对应的文本。 在生成了大量的数据后，采用了两个任务对预训练模型进行进一步的fine-tune, 分别是 MLM （经典的训练任务）， SSP objective（预测一个列是否出现在文本中，并且是什么操作中出现的）。

在WIKISQL ， WIKITABLEQUESTIONS ，SPIDER数据集上都取得了很好的效果。

本文后续提出的几个可以详细思考的点，1. Pre-training objectives ， 同时MLM和SSP比两个单独使用效果要好很多。2. Generalization， 尽管是text-to-sql任务上训练的，但是却可以在其他的一些semantic parsing 任务上取得不错的效果。 3. Pre-training time and data： 他们的实验表明仅仅需要相对比较小的数据进行fine-tune就可以，需要的epoch也比较小，这样可以让BERT获得新的能力同时保留其encoding能力。  这里还有一点是，GRAPPA是用相对规则文本进行训练的，这样容易影响模型的encoding性能，但是如果使用预训练模型生成的数据可能会好一点？4.  Pre-training vs. training data augmentation  本文的实验表明采用预训练的方式是更好的选择。

### Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training 

Tag:预训练模型  Tag:语义解析

AAAI 2021

https://ojs.aaai.org/index.php/AAAI/article/view/17627

这也是一个很早就读过的文章了，再重新总结一下～

目前大多数的预训练模型都是适合在通用场景下，但是在应用到text-to-sql模型时会遇到几个问题： 分别是 1. fail to detect column mentions in the utterances 2. fail to infer column mentions from cell values 3. fail to compose complex SQL queries.    所以本文提出了一个框架，能够生成增强数据，然后再进行预训练。 

首先是预训练部分，本文一共有4个预训练任务，分别是 1. Column Prediction (CPred) ： 预测一个列是否在文本中出现 2.  Column Recovery (CRec):  从cell value 推断 column。（之前有一点低估了这个任务，现在看来还要重视一下）   3. SQL Generation (GenSQL):  直接生成SQL语句  4.  Masked Language Model(MLM)： 和经典预训练训练的方法一样。 

数据生成： SQL和表格都是爬虫获得的，SQL-to-Text 直接采用了 BART 模型，没有采用什么预训练的机制。  table-to-text:  使用了一些 control code 配合table生成了text。

实验结果表明， Column Recovery (CRec) 的效果竟然还是挺好的。

### Calibrate Before Use Improving Few-Shot Performance of Language Models

Tag:预训练模型

http://arxiv.org/abs/2102.09690 

ICML 2021

探究 prompt 不稳定性的paper。作者指出 GPT-3 incontext learning (prompting) 的方案在 few-shot 场景下波动很大的三个因素在于：

- majority label bias: 示例中有类别的不均衡造成；
- recency bias: 示例中连续重复的 label 也会对 prediction 造成 bias;
- common token bias: 预测的 label token 在 MLM 任务中的频率也会对预测结果造成影响。

作者提出基于上下文的 calibration 策略，在给定 prompt 的情况下，先将输入的带预测文本置成 N/A 得到对应的概率，然后用这个分布再对待预测文本的预测进行校正。作者发现这种方案能够大比较明显地提高模型的预测效果，同时也对 variance 起到了一定的降低的效果。

### Zero-Shot Text-to-SQL Learning with Auxiliary Task 

Tag:Text2SQL  Tag:元学习

https://ojs.aaai.org/index.php/AAAI/article/view/6246

AAAI 2020

github地址 https://github.com/JD-AI-Research-Silicon-Valley/auxiliary-task-for-text-to-sql

本文研究 如何在 zero-shot 下进行 text-to-sql 工作，  具体来说，是在 wiki-SQL 数据集下，按照作者的说法， 在wikiSQL 数据集默认的划分中，测试集中70%的数据库在训练集中是见过的，这样不太符合设定。 （其实spider数据集也已经是 zero-shot setting 了，但本文没在这个数据集下实验）。

具体来说，首先使用了两个LSTM网络分别对问题和column进行编码， 然后使用了一个BiAttn 机制，对column和问题进行更好的互编码。  在decode的时候针对 select, agg 和 where分别设计了不同的解码模块。 同时设计了一个辅助任务来更好的对齐 column 和 问题， 辅助任务具体是先通过 sequence labeling 识别句子中和列对应的地方，然后使用一个 pointer网络上进行学习。

实验效果上，在WikiSQL 标准数据集下取得了3%的进步， zero-shot setting 的数据集下有 5%的提升。

### Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning 

Tag:Text2SQL

https://www.aaai.org/AAAI21Papers/AAAI-6324.ChenY.pdf

AAAI 2021

Github地址:   https://github.com/qjay612/meta_learning_NL2SQL

本文同样是去解决 zero-shot text-to-sql 的问题，没有使用其他的数据，仅仅使用wikiSQL 中的数据，引入了数据库内容并且使用 meta-learning 的方法在 zero-shot setting 下取得了很好的效果。

具体来说，在基础模型上，也是先给了一个skeleton，然后把整个任务分成了6个子任务 （Select-Column(SC),	 Select-Aggregation(SA), 	Where-Number(WN), Where-Column(WC), Where-Operator(WO)， Where-Value(WV)。   在引入数据库内容上，由于有的数据库内容会太多，不可能全部进行编码，所以本文首先了进行了筛选，筛选出了一些比相关的数据库内容。 在编码方式上，把问题和数据库column一同放到BERT中进行编码，然后把数据内容通过char embedding进行编码， 然后Ec 和 Eq 分别输入到6个不同的子任务模块中。  元学习方法就有点类似经典的方法了，不再多介绍了。

最终在多个wikiSQL 和 ESQL 数据集的 full, few-shot 和 zero-shot setting 下都取得了最优的效果。

### Prefix-to-SQL Text-to-SQL Generation from Incomplete User Questions 

Tag:Text2SQL

https://arxiv.org/pdf/2109.13066.pdf

提出了一个新的数据评价方式，从已经存在的各个数据集中进行构造，根据用户输入到一半的文本推断出SQL语句，可能只需要满足top-k准确率就可以了。

在各个数据集上的准确率目前是参差不齐的。

### DoT An efficient Double Transformer for NLP tasks with tables 

Tag:表格相关

ACL 2021 findings

https://aclanthology.org/2021.findings-acl.289.pdf

github地址： https://github.com/google-research/tapas

本文主要是来提升有table输入的问题的计算效率问题， 设计了一个double transformer,  设计了一个double transformer,  其中第一个transformer是用来选择token的，计算出一个pruning score，作为下一个transformer时的一个输入。

在几个文本和表格合并输入的任务上都取得了不错的效果， 没有损失精度的情况下训练和推断速度都大大提升了。   主要实验的数据集WIKISQL （Text-to-SQL）, TABFACT (表格事实验证)， WIKITQ （表格问答）。

### Understanding tables with intermediate pre-training 

ACL 2020  findings

https://aclanthology.org/2020.findings-emnlp.27.pdf

研究的主要任务是 Table entailment， 判断一个句子是否能被表格中的内容支持， 是一个二分类任务， 类似于Fact-check任务，作者也是用了数据增强预训练的方式来做的。

### Re-examining the Role of Schema Linking in Text-to-SQL 

Tag:Text2SQL

https://aclanthology.org/2020.emnlp-main.564.pdf

github地址：https://github.com/WING-NUS/slsql

EMNLP 2020

在这篇工作之前，cross-domain text-to-sql任务中的 schema-linking 模块都被认为是一个比较小的模块， 本文主要研究 schema-linking 这个模块带来的影响。   

本文首先在 spider 数据集上对schema-linking的信息进行了标注，具体方法是结合了自动标注和人工标注。 首先进行自动标注匹配，然后进行人工筛选。 作者发现，影响自动标准准确率最大的障碍是一些缩写，一些string缩写后和之前不能完全匹配。

在模型方面，本文提出了一个相对来说简单的模型，更好的测试schema-linking的准确度， encoder部分还是直接使用bert编码，然后单独设计了一个 Schema Linking Learning， 学习question token和 schema token 之间的link, 之后是 Schema-aware Representation， 根据上个步骤学到的 Schema Linking 结果，进行一个加权求和表示。

在实验上，作者在几个下面几个设定下进行了实验： base model:  只进行了 encoder和decoder部分，没有进行任何的 scema-linking 模块。  auto：使用自动标注的 schema-linking模块进行训练。 hard reference： 不加权了，直接把概率最大的两个 concat 起来。oracle：不使用schema-linking模块，直接把oracle的结果拿来，同时包括了dev 集和train集。  在auto上是取得了最佳效果，能提升10个点以上。 本文的结论也就是说 schema-linking是一个十分重要的模块了，能够让一个简单的BERT模型取得十分好的效果。

### Translate & Fill Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data

Tag:语义解析

https://arxiv.org/pdf/2109.04319.pdf

总体来说是一个帮助semantic parsing跨语言zero-shot进行学习的工作， 目前的在英语上很多的 semantic parsing工作已经做的比较好了， 但是在其他语言上面临的问题是确实监督训练数据集。 

本文的大概思想是设计了一个框架，借助了跨语言的预训练模型，采用了一种先翻译句子，再让预训练模型去fill的过程来学习，再zero-shot 的情况学到一种不错的跨语言 sementic parsing 工作。

### Context-Aware Attention Network for Image-Text Retrieval

Tag:视觉相关

CVPR 2020

https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Context-Aware_Attention_Network_for_Image-Text_Retrieval_CVPR_2020_paper.pdf

这篇文章还是针对于每个 image-text pair 去做处理了， 打的核心的点是说可以同时做到识别跨模态的相似度和同一个模态内部的相似度。

这篇文章同时写了一些 related-work 部分，对image-text retrieval 的两个方向进行了介绍，分别是Global embeddings based methods（把图片和文本建模到同一个空间中去）， Local fragments based methods （文本的细节和图片的细节可以去对应）。

### Table2Vec: Neural Word and Entity Embeddings for Table Population and Retrieval 

Tag:表格相关

https://arxiv.org/pdf/1906.00041.pdf

SIGIR2019 short paper

这篇文章使用 table 做了一个预训练模型，大概使用了类似于word2vec的方法，最终在Row Population， column population, Table Retrieval  三个下游任务上都取得了不错的效果。

### CPT COLORFUL PROMPT TUNING FOR PRE-TRAINED VISION-LANGUAGE MODELS 

Tag:Prompt Tag:视觉相关

https://arxiv.org/pdf/2109.11797.pdf

尽管视觉语言模型已经取得了非常的大的进展，但是在预训练和fine-tune的时候仍然有一个比较大的gap，这就导致了fine-tune的时候也需要相对比较大的数据量，所以本文提出了CPT（Cross-modal Prompt Tuning ）也可以叫做Colorful Prompt Tuning， 把visual grouding 任务变成 fill-in-the-bank 任务。 

具体来说，为了把文本图片统计进行建模，文章把图片中的物体渲染上了不同的颜色，然后在文本中就可以使用颜色来代指图片中的具体部分，这样就可以很方面的时候自然语言来构造prompt,  比如 the horse watched by the woman is in  [mask] color.  这样可以选择mask文本的不同部分，就可以完成建模中图片中元素关系任务。

最终在zero-shot 和 few-shot setting 上取得了比较大的进步。

### NSP-BERT A Prompt-based Zero-Shot Learner Through an Original Pre-training Task Next Sentence Prediction 

Tag:预训练模型

https://arxiv.org/pdf/2109.03564.pdf

github 地址 https://github.com/sunyilgdx/NSP-BERT

之前的 prompt-based 的相关工作，都是都是探究更好的利用的预训练模型，很少有关注具体的预训练任务的。 这篇工作充分发挥了NSP任务的潜力， 并且启发我们哪怕局限在Prompt-based，其研究思路还有很大的发散空间。

所谓NSP任务，并不是真的去预测下一句，而是给定两个句子，判断这两个句子是否相邻。相应地，NSP-BERT的思路其实很简单，以分类问题为例，就是把输入视为第一句，然后将每个候选类别添加特定的Prompt作为第二句，逐一判断第一句与哪个第二句更加连贯。可以发现NSP-BERT思路跟PET很相似，其实Prompt-based的工作都很容易理解，难的是如何首先想到这样做。 

最终的实验表明，NSP-BERT的 prompt learning 取得了十分不错的效果，在 few-shot learning 的诸多baseline上都有很好的效果。

### TAPEX Table Pre-training via Learning a Neural SQL Executor 

Tag:Text2SQL

ICLR 2022 投稿文章

https://openreview.net/pdf?id=O50443AsCP

又是一篇 table 预训练的文章， 针对表格的预训练方法一般有两个方面需要去考虑，一个是如何去获取数据集，还有就是如何去设计预训练的任务。  之前的方法要么是采用爬虫数据，或者是自己生成数据， 作者任务这两种方式都是有问题的。 

在本文中，采用的预训练任务非常简单，就是给一个SQL语句和对应的表格，让模型去输出答案，即让模型去学习一个SQL解析器，这样做的优点是可以有无穷多的数据可以使用，因为SQL是可以无限采样的，并且数据质量也是很高的。 作者就在BART模型的基础上进行预训练，最终在多个数据集上都达到了非常高的执行准确率。 

### On the Importance of Word Order Information in Cross-lingual Sequence Labeling 

https://arxiv.org/pdf/2001.11164.pdf

AAAI 2021

在一种语言上训练的跨语言模型可以很轻松的迁移到其他的语言上，但是本文发现，因为文本的顺序的差别在不同的语言中出现的十分频繁，一旦跨语言的模型在一种语言训练时overfit到语言的顺序，可能会对其他语言上的性能产生影响。 所以本文提出了假设，减少单词顺序信息的影响可以提升模型在其他语言上的泛化性。 为了验证这个猜想，作者采用了三种不同的方法进行验证。

1. Order-Reduced Transformer， 消除 transformer 中的 position embedding, 并且使用了一个一维的cnn对位置进行编码。 替换了一些baseline 中的对应模块，发现在英文（源语言上的效果下降了），但是在其他语言上的效果上升了。
2.  Shuffling Word Order， 直接对单词的顺序进行shuffle,   输入到模型中，在英语的效果下降了，在其他语言大多数效果也下降了。
3. Order-Agnostic Positional Embeddings ， 使用 M-BERT 中的固定位置编码， 并且不更新这个编码，效果也是大多数都不好。
4. 对M-BERT不调节位置进行fine-tune, 在英语上下降，在其他语言上都有提升。

### Multiplicative Position-aware Transformer Models for Language Understanding

Tag:Transformer


https://arxiv.org/pdf/2109.12788.pdf


这篇文章主要研究在 transformer 中如何去引入一些 position 机制。


由于 transformer 全部采用的是 self-attention layer, 必须要显式的引入位置信息，本文总结了目前方法中的引入位置信息的方式，并且在自己的实现下进行实验，同时提出了一个比较好的位置编码方法。 

目前可以采用的位置编码方式有

1.  transformer原文中提出的相对位置编码。
2.  Self-attention with relative position represen- tations. 文章中把距离信息引入self attention 的计算公式， $e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}+a_{i j}\right)^{T}}{\sqrt{d_{z}}}$
3.  Exploring the limits of transfer learning with a unified text-to-text trans- former.  同样放入 self-attention 公式，以加法的形式放入， $e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}+a_{i j}}{\sqrt{d_{z}}}$
4. Improve transformer models with better relative position embeddings， 放入self-attention 公式，以乘法的形式放进去$e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T} \times a_{i j}}{\sqrt{d_{z}}}$， 同时还有 M4版本$e_{i j}=\frac{\left(x_{i} W^{Q}\right) \cdot\left(x_{j} W^{K}\right)+\left(x_{i} W^{Q}\right) \cdot a_{i j}+\left(x_{j} W^{K}\right) \cdot a_{i j}}{\sqrt{d_{z}}}$， 
5. Deberta: Decoding-enhanced bert with disentangled attention. 文章中分开引入self-attention 公式的方式$e_{i j}=\frac{\left(x_{i} W^{Q}\right) \cdot\left(x_{j} W^{K}\right)+\left(x_{i} W^{Q}\right) \cdot\left(a_{i j} W^{R}\right)+\left(x_{j} W^{K}\right) \cdot\left(a_{i j} W^{T}\right)}{\sqrt{3 d_{z}}}$
6. Rethink- ing positional encoding in language pre-training. 中的先求self-attention 再求和方式， $e_{i j}=\frac{\left(x_{i} W^{Q}\right) \cdot\left(x_{j} W^{K}\right)+\left(p_{i} U^{Q}\right) \cdot\left(p_{j} U^{K}\right)}{\sqrt{2 d_{z}}}+a_{i j}$
7. 本文最后提出的是 $e_{i j}=\frac{\left(x_{i} W^{Q}\right) \cdot\left(x_{j} W^{K}\right) \times\left(x_{i} W^{Q}\right) \cdot a_{i j} \times\left(x_{j} W^{K}\right) \cdot a_{i j}}{\sqrt{d_{z}}}$

本文提出了相对位置编码在多个任务上都取得了不错的效果。 

同时也显而易见的是，最简单的相对位置编码效果是最差的。

感觉如果把文本作为图建模的话这些方法都是可以使用的。

### Semi-Supervised Learning for Neural Machine Translation 

Tag:机器翻译

https://aclanthology.org/P16-1185.pdf

ACL 2016

这篇文章做半监督的机器翻译，机器翻译领域的一个很大的瓶颈是很难获得很大量的标注文本， 所以如何利用好单语言语料库来做半监督学习提升机器翻译模型的性能就成了一个问题。

这篇文章提出了使用autoencoder的方法，具体来说，就是从y->x->y 重构的过程， 但是和标准的autoencoder不同的是，这里的梯度是截断的，但是由于x的空间是无限的无法进行优化，所以简化了搜索空间，使用top 10 的 x 作为全部空间（beam search）。 整体的目标函数就是标准机器翻译loss+两个方向的autoencoder loss。

最终的实验效果， 在中文英文翻译的设定下，在几个数据集上大都取得了进步。

### CONDITIONAL SET GENERATION USING SEQ2SEQ MODELS

ICLR 2022 投稿论文

https://openreview.net/pdf?id=q23I9kJE3gA

本文解决的是 sequence2set 的任务，即给一个文本，生成其对应的集合，常见的任务包括 open-entity typing 和 fine-grained emotion classification。

之前基本都是把这个任务完全放在seq2seq的设定下来做，但是这样做有两个问题， 1是集合本身是无序的，但是seq2seq会考虑顺序，这样就会带来不必要的特征 2. 集合中的元素数量没有显式的在模型中建模。 

于是本文采用的做法是，给set中的所有元素做一个拓扑排序，设定一个全局的顺序，生成一个拓扑网络， 通过拓扑排序可以一个一个集合采样出多种顺序，这样顺便还达到了数据增强的效果，同时在集合在最前面加上集合的数量。 

实验效果还是出奇的好，效果提升了10个点。

### ELECTRA PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS 

Tag:预训练模型

BERT模型采用的 mask language 模型只利用了一部分语言特征，也就是mask掉的那15%，这样被认为计算利用率不高。

所以本文提出了 ELECTRA 是 Efficiently Learning an Encoder that Classifies Token Replacements Accurately的简称， 设计了一个新的预训练任务，即对原本的句子使用一个小的MLM模型进行替换，再让ELECTRA去判断哪个词被替换了，这样就大大的提升了计算利用率，把所有的文本都利用到了，也有一点对抗学习的感觉。

最终的实验效果上，ELECTRA 模型的训练时间显著减少，并且取得了比BERT模型更优秀的效果。

### End-to-End Object Detection with Transformers 

Tag:视觉相关

https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf

ECCV 2020

DEtection TRansformer， 一个使用 transformer的方法来做 object detection 的尝试。 

具体的做法是，首先使用CNN backbone对图片进行的编码，生成的编码压平后加入绝对位置编码作为transformer encder的输入，transformer decoder的输入也是特殊的position embedding,  同时接受 encoder的输入， FFN分类后产生框的中心位置，宽， 高和类别。

在多个backbone, 多个数据集上都取得了最优效果。

### PIX2SEQ A LANGUAGE MODELING FRAMEWORK FOR OBJECT DETECTION 

Tag:视觉相关

https://arxiv.org/pdf/2109.10852.pdf

这个工作也是采用 transformer 来去解决计算机视觉中的目标检测问题，和上一篇DETR方法不同，这篇工作的输出是一个自回归的序列。

具体来说，也是用一些backbone的输出作为transformer encoder的输入，然后重点是采用了一些数据增强的技术，同时在序列解码时，采用了一些 sequence construction&augmentation  技术， 个人理解是直接naive的做效果并不好，这样确实提升了指标。

最终的实验效果，和DETR 类似，但是直接以序列的方式建模目标检测的思路还是让人眼前一亮的。 

### TextAttack A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP

Tag:对抗学习

https://aclanthology.org/2020.emnlp-demos.16.pdf

EMNLP 2020 demo

这篇文章是提出了一个方便NLP研究者使用的文本攻击工具，名字是TextAttack。

TextAttack 把文本攻击分成了四个模块，分别是goal function（判断攻击是否成功，比如是否导致类别发生变化), A set of constraints (产生的文本必须满足这个限制，比如embedding的相似度不能太大)， transformation（核心方法了， 给一个输入，产生一堆候选）， search method（搜索方式，如何从候选中找出最好的攻击文本）

同时这个框架也集成了之前的好多工作，具有的特点是模型无关（可以在任务模型上应用），并且可以帮助进行数据增强和对抗学习。

### BERT-ATTACK Adversarial Attack Against BERT Using BERT

Tag:对抗学习

https://aclanthology.org/2020.emnlp-main.500.pdf

EMNLP 2020

这篇文章使用BERT模型来做文本的对抗攻击，方法也非常的简单，先找到句子中最关键的词，再对这个词进行相近的替换，直到模型预测的label发生改变。

找关键词步骤，对句子中每个词mask，比较前后输出的logits差别。

替换词步骤，对同义词表中的词在mask language models 中进行排序。

最终发现这种比较简单的方法能造成一些数据集非常剧烈的效果下降。

### Combating Adversarial Misspellings with Robust Word Recognition

Tag:对抗学习

https://aclanthology.org/P19-1561.pdf

ACL 2019

这篇文章算是一篇应对 文本攻击的 方法，但是情景是比较简单的，应对的情景是文本有Misspellings， 一些词语的拼写错误，导致了分词的时候产生UNK ID，最终模型预测错误的情况。

具体的解决方法也比较的简单粗暴，就是在具体的模型前增加一个word recognition 的模型，能够识别错误词并且尽可能的恢复到有正常ID 的词上。

最终的实验也说明该方法在一定程度上起到了帮助。

### SeaD End-to-end Text-to-SQL Generation with Schema-aware Denoising 

Tag:Text2SQL

https://openreview.net/pdf?id=Dgx157I8729

ACL 匿名投稿

这篇文章做的还是WikiSQL数据集上的实验，主要做法是采用了一个seq2seq的方法累进行text-to-sql任务，同时加入了一些denoising任务。

seq2seq 中encoder部分还是拼接了问题和column，同时增加了一些特殊符号token的处理，在decode的时候使用了一个Transformer with pointer,  来方便进行更好的schema-linking。 关于Schema-aware Denoising，主要进行了两个任务，一个是Erosion， 对原本的column输入进行 	Permutation，Removal 和 addition,  同时removal的时候如果去除了关键的列，那么也会去把对应SQL中的列去除。 另外一个shuffle任务，对原本的句子打乱进行还原。 在inference decode的时候，也采用了一个 execution guided的方法，借助SQL 的执行引擎来帮助生成语法正确的SQL语句。

实验效果上也是高于之前的各种方法，但也高的不是特别多。

### Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment 

Tag:Text2SQL

https://openreview.net/pdf?id=-B3vVVeVyTr

ACL 2022 匿名投稿

这篇文章主要研究Text-to-SQL 中的组合泛化性问题， 对之前的SPIDER数据集进行了更加细粒度的标注，产生了SPIDER-SS数据集，后面又采用了一些方法来帮助解决问题。

### RoBERTa A Robustly Optimized BERT Pretraining Approach 

Tag:预训练模型

https://arxiv.org/pdf/1907.11692.pdf

虽然很出名，但是好像没中稿？

这篇文章更多的是一种针对BERT模型的一个工程上的优化，最终的效果确实有了提升，但是本质上没有创新点的，这也是这篇文章最终被拒掉的理由。

具体来说，这篇文章提出的几个优化点分别是：

1.  训练更长的时间，在更多的数据上使用更大的batch。  这个点比较显然，这篇文章直接测试2k的bacth size也太强了。
2.  去掉 next sentence prediction 任务。   这篇文章通过实验说明这个任务作用不大。
3.  在更长的序列上进行训练。  
4.  根据训练数据动态改变mask方式。  训练的时候动态进行mask, 对每个数据可以产生不同的mask数据。

### Disentangled Sequence to Sequence Learning for Compositional Generalization 

Tag:语义解析

https://arxiv.org/pdf/2110.04655.pdf

这篇文章研究 seq2seq model 为什么不能组合泛化的原因， 一个很重要的假设是， seq2seq model 学习不到局部的context-free,一个关系的编码容易受到另一个关系的影响，这个叫做entangled（相互纠缠）。

一个很简单的预实验是，包含有两个的关系文本，如果对另一个关系进行增强，把所有可能的序列都生成进行增强，那么在预测当前关系时就能学到一个context-free的，当前关系几乎就能100%的预测了。

本文后续设置了一个方法，多步encode， 每次decode一个文本时，都重新去encode文本，避免针对具体target的纠缠。

在几个semantic parsing数据集上取得了比较好的效果。

### Incorporating Extra Knowledge to Enhance Word Embedding 

Tag:外部知识引入 Tag:词向量

IJCAI-20  survey paper

https://www.ijcai.org/Proceedings/2020/0686.pdf

这篇文章是针对如何把外部知识引入词向量中的一个的一个综述性工作。

经典的词向量一般能够很好的建模词之间的共现信息，但对于NLP系统来说也很重要的一些其他信息就很难建模，所以有些情况下需要外部的知识更好的辅助词向量的建模。 主要作用有 1. 提升从文本中学到的词向量的质量。 2. 增加领域知识让领域特定的NLP任务做的更好 3. 解决在特定领域上的数据稀疏问题。 

一共总结了近10年来的21篇工作。 把数据来源分成了 1. Wikipedia  大型文本 2.  Wordnet 建模了词之间的关系，比如同义词表 3. Freebase 包含了关系3元组。  4. Domain Dictionary.  包含特定领域的知识 5. 一些NLP的工具等。 

知识的类型包括： 1. 文本知识 2. 词法知识 3. 类别知识 4. 关系知识 5. 知识图谱知识。

方法的分类有四个维度

1. 静态或者动态的词向量： 是否引入context 的信息。
2. 共同优化或者后处理： 是否是先学习词向量然后再增加约束。
3. 单独编码或者耦合编码
4. 每个词是否有多个语义： 多义词的情况。

最终作者也提出了一些未来的可能方向：1. 可解释性问题 2. 多种知识共同引入 3. 引入文档级别 4. 计算高效性

### Self-Supervised Learning for Contextualized Extractive Summarization

Tag:自监督学习 Tag: 摘要

https://aclanthology.org/P19-1214.pdf

ACL 2019

现在的文本摘要模型基本都是从头开始训练， 这样不能很好的提取文档级别的上下文。

本文为了解决这个问题，引入了三个辅助任务来获得文档级别的上下文信息。

具体来说说三个自监督任务分别是  1. Mask： 随机掩盖一些句子，然后从一些候选句子中选择正确的句子。  2. Replace： 随机的从其他的文档中替换一些句子，然后来判断这些句子是否是被替换的。 3. Switch： 在同一个文档内替换一些句子，然后来判断这些句子是否是被交换过顺序的。 

最终增加了各个任务后确实取得了提升。

### Mind the Style of Text Adversarial and Backdoor Attacks Based on Text Style Transfer

Tag:对抗学习

https://scholar.google.com/scholar?cluster=3350315178608645607&hl=zh-CN&as_sdt=0,5

这篇文章也是做针对自然语言处理模型的攻击的，主要研究文本风格对自然语言模型的攻击效果，因为文本风格是一个和具体模型无关的特征，模型一般不会去关心这个点，所以文本风格比较适合拿来做攻击。

具体来说，本文使用了STRAP 模型来产生不同风格的文本，该模型主要是采用了GPT2模型在back-translation自动产生的数据上进行训练，并且支持产生多种不同的风格。

在攻击方法上，本文同时尝试了Adversarial Attacks， Backdoor Attacks， Adversarial Attacks 是只改inference阶段。不断生成同语义但是不同风格的文本，直到攻击成功， Backdoor Attacks是在训练的阶段，把语义风格转换后的数据加入到训练集中， 让模型产生偏差。

最终在大多数case上都有攻击效果。

### A MUTUAL INFORMATION MAXIMIZATION PERSPECTIVE OF LANGUAGE REPRESENTATION LEARNING 

Tag:词向量 Tag:自监督学习

https://arxiv.org/pdf/1910.08350.pdf

ICLR 2020

这篇文章对各种表示学习方法提出了一个统一的视角，使用一个最大化互信息的理论框架来解释表示学习模型。把skip-gram, bert, XLNet 都统一到了这个模型当中，并且指出skip-gram 和BERT其实是最大化一个相似的目标式。  作者使用一个直接优化互信息的自监督方法进行实验， 能够取得和BERT类似的效果。

### Unsupervised Pretraining for Sequence to Sequence Learning

Tag:机器翻译 Tag:预训练模型

ACL 2017

https://arxiv.org/pdf/1611.02683.pdf

相对比较早期的关于预训练的文章，这篇文章针对机器翻译做了一些预训练，一般机器翻译的架构都是encoder-decoder架构，这篇文章假设在encoder和decoder分别独立的情况下， encoder和decoder应该分别是两种语言的语言模型。

所以预训练的方式就是将encoder和decoder分别在两种语言上进行预训练模型的pre-train, 最后在有标注数据集上进行fine-tune。

实验结果在BLEU指标上提升了1.3个点。

### Transformer-XL Attentive Language Models Beyond a Fixed-Length Context

Tag:Transformer 

ACL 2019

https://arxiv.org/abs/1901.02860

本文主要研究如何赋予编码器捕获长距离依赖的能力。在基于Transformer的模型中，允许词之间直接建立联系【self-attention】，能够更好地捕获长期依赖关系，但是还是有限制。

Transformer最长建模512，更长的可能就会失去一些语义信息，所以提出片段级递归机制(segment-level recurrence mechanism)，引入一个记忆(memory)模块（类似于cache或cell），循环用来建模片段之间的联系。 并且提出了提出相对位置编码机制(relative position embedding scheme)，代替绝对位置编码。

在语言模型上取得了非常优异的效果。

### XLNet Generalized Autoregressive Pretraining for Language Understanding

Nips 2019

https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf

Tag:预训练模型

也是一个经典的预训练模型，基于AR的预训练模型只有从左到右的上下文信息。 基于AE的预训练模型 pretrain 和 train不一致。 

本文结合AR LM和AE LM，在Transformer-XL的基础上提出generalized autoregressive method，XLNet。

所有的分解序列作为一个集合，对所有采样序列，XLNet按照AR LM的计算方式求对数似然期望的极大值。通常，当前token的上文包含left和right的tokens：比如原始序列为1-2-3-4，分解序列中采样一个为2-4-1-3，那么如果当前token为3，XLNet的方式就可以看到所有的信息【当然这也是理想情况】，而AR LM只能看到1和2。

引入Transformer-XL的segment recurrence mechanism和relative encoding scheme。

引入Masked Two-Stream Self-Attention解决PLM出现的目标预测歧义【the ambiguity in target prediction】问题。举个例子，比如分解序列中采样一个为2-4-6-1-3-5的序列，假设要预测位置[1]的token，按照经典的Transformer来计算next-token的概率分布，位置[1]的token的概率就是通过[2,4,6]位置上的tokens来计算。但是如果以这种方式去预测next-token，这对[3,5]的预测就会产生影响，因为如果[1]的预测出现错误会把错误传给后面。对后面每一个token的预测，需要建立在之前token都已知的条件下。因此本文计算了两个self-attention计算方式，一个mask当前词，attention值记为g；一个已知当前词，attention值记为h。最后假设self-attention一共有M层，用第M层、t时刻的g_t，去预测词x_t。

最终，在非常多的任务上都取得了很大的提升。

### Understanding Back-Translation at Scale 

Tag:机器翻译

ACL 2018

https://aclanthology.org/D18-1045.pdf

这篇文章对 beam search 进行了更加详细的研究，主要在有大规模数据的情况下。 研究了5种设定下的实验

1. sampling: 直接从翻译模型中输入y, 采样得到一个x’ （随机采样）
2. beam search  根据beam search得到x‘, 这样x’的质量较高
3. beam + noise 在beam search的过程中, 添加给token添加随机噪声
4. greedy 根据输入y, 翻译时每一步采用最好的一个token, 翻译得到x’
5. top 10 根据输入y, 翻译时每一个在前10个最可能的token中采样得到下一个token

在平行预料比较多的情况下，带有噪音的采样方法效果好一些(beam+noise, sampling), 原因大致可以解释为去噪的影响, 噪声多了, 效果会好一些。 在平行预料少的情况下，可能还是beam search效果更好一点。

### SQuAD: 100,000+ Questions for Machine Comprehension of Text 

Tag:数据集 Tag:问答

https://arxiv.org/pdf/1606.05250.pdf

ACL 2016

著名的 SQuAD 数据集， 机器阅读理解领域的重要数据集。 包含了在536篇文章上的 107,785 个问题-答案对。  

包含了非常多种的答案类型，有Date 、Other Numeric 、 Person 、Location、Other Entity、Common Noun Phrase、Adjective Phrase、Verb Phrase、Clause 、Other  等类型。 并且很多回答都需要一定的推理，因为问题和原文件的语法结构是有一些不同的。

目前这个数据集感觉已经被刷爆了，human performance 是 91.221， 但是模型已经做到了95.719。


### Know What You Don’t Know: Unanswerable Questions for SQuAD

Tag:数据集 Tag:问答

https://arxiv.org/pdf/1806.03822.pdf

ACL 2018 best paper

SQuAD2.0 数据集，和SQuAD1.0 数据集相比，增加了不可回答的问题，需要模型去主动的判断提出的问题是否能够在段落中找到依据。 把之前的数据集和 53775 个关于相同段落的、无法回答的新问题相结合。

SQuAD2.0 数据集比1.0更有挑战性，在SQuAD1.0上最好的模型在 SQuAD2.0上只有66.3% 的 F1 得分。

但这个数据集目前也已经刷爆了，人类水平是89.452，目前的模型已经达到了93.214。

### The Power of Prompt Tuning for Low-Resource Semantic Parsing 

Tag:Prompt

https://arxiv.org/pdf/2110.08525.pdf

这篇文章主要是一个实验论文，主要研究了两个点。

1.  Prompt Tuning  在低资源语义解析上的作用，实验发现确实是有效果的，在低资源上稳定超过 fine-tune (必须在 large 模型)， 模型越大， prompt的效果越好。  
2.  Canonical 形式和 Meaning 形式的区别，作者发现，在 T5-large的设定下， 趋势区别是很小的，T5-small 和 T5-base差距还是比较大。  但总体上 Canonical 形式并没有之前模型说的那么大的优势。


### CLINE Contrastive Learning with Semantic Negative Examples for Natural Language Understanding

Tag:对抗学习 Tag:预训练模型  Tag:自监督学习

ACL. 2021

https://arxiv.org/pdf/2107.00440.pdf

目前的很多预训练模型容易收到轻微扰动的影响， 比如同义词替换可能使模型产生相反的输出（adversial case), 反义词替换可能使模型仍然有相同的输出（contrastive case）， 仅仅使用adversial training 的方法可能会使模型在 adversial case 上表现比较好，但是在 contrastive case 上表现不佳，所以本文提出了一个对比学习的方法来改善这一点。

首先使用同义词替换构造正例，然后使用反义词替换构造对比例子，这样使用三个loss对预训练模型进行重新调整学习，一个是MLM loss，一个是类似electra 的判断单词是否被替换过的loss,还有一个是对比学习loss。

最终在各个数据集的contrastive 和 adversial case上都能取得最好的效果。







